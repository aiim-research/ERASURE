{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from erasure.utils.config.local_ctx import Local\n",
    "from erasure.utils.config.global_ctx import Global \n",
    "from erasure.core.factory_base import ConfigurableFactory\n",
    "from erasure.data.datasets.DatasetManager import DatasetManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-07 12:31:28,-533866475 | INFO | 3841452 - Creating Global Context for: configs/survey/IMDB.jsonc\n",
      "2025-02-07 12:31:28,-533866406 | INFO | 3841452 - Setting seeds to: 0\n",
      "2025-02-07 12:31:28,-533866368 | INFO | 3841452 - \u001b[91mCaching System: True.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/NFSHOME/adangelo/miniconda3/envs/representer/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-07 12:31:30,-533864578 | INFO | 3841452 - Created Configurable: erasure.data.preprocessing.reshape_x_z.reshape_x_z\n",
      "2025-02-07 12:31:36,-533857903 | INFO | 3841452 - Instantiating: erasure.data.preprocessing.tokenize_text.TokenizerWrapper\n",
      "2025-02-07 12:31:37,-533857085 | INFO | 3841452 - Created Configurable: erasure.data.preprocessing.tokenize_text.TokenizeX\n",
      "2025-02-07 12:31:37,-533857084 | INFO | 3841452 - Created Configurable: erasure.data.data_sources.HFDataSource.HFDataSource\n",
      "2025-02-07 12:31:37,-533857083 | INFO | 3841452 - {'class': 'erasure.data.data_sources.HFDataSource.HFDataSource', 'parameters': {'path': 'fgiobergia/imdb-id', 'data_columns': ['text', 'movie_id'], 'label': 'score', 'to_encode': ['movie_id', 'score'], 'preprocess': [{'class': 'erasure.data.preprocessing.reshape_x_z.reshape_x_z', 'parameters': {'keep_as_x': [0], 'move_to_z': [1]}}, {'class': 'erasure.data.preprocessing.tokenize_text.TokenizeX', 'parameters': {'tokenizer': {'class': 'erasure.data.preprocessing.tokenize_text.TokenizerWrapper', 'parameters': {'tokenizer': 'bert-base-uncased', 'max_length': 512, 'padding': 'max_length', 'truncation': True, 'return_tensors': 'pt'}}, 'max_length': 128}}], 'configuration': '', 'to_normalize': [], 'classes': -1}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:01<00:00, 14361.92 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:01<00:00, 13886.86 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:03<00:00, 15159.83 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:01<00:00, 13041.78 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:02<00:00, 12482.12 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:04<00:00, 12373.02 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-07 12:31:56,-533838087 | INFO | 3841452 - <torch.utils.data.dataset.ConcatDataset object at 0x7f391ab5d3d0>\n",
      "2025-02-07 12:31:56,-533838086 | INFO | 3841452 - Instantiating: erasure.data.datasets.DataSplitter.DataSplitterPercentage\n",
      "2025-02-07 12:31:56,-533838007 | INFO | 3841452 - ['all', 'all_shuffled', '-']\n",
      "2025-02-07 12:31:56,-533838006 | INFO | 3841452 - Instantiating: erasure.data.datasets.DataSplitter.DataSplitterByZ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering Data: 100%|██████████| 10/10 [02:38<00:00, 15.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-07 12:34:35,-533679314 | INFO | 3841452 - ['all', 'all_shuffled', '-', 'forget', 'other_ids']\n",
      "2025-02-07 12:34:35,-533679312 | INFO | 3841452 - Instantiating: erasure.data.datasets.DataSplitter.DataSplitterPercentage\n",
      "2025-02-07 12:34:35,-533679309 | INFO | 3841452 - ['all', 'all_shuffled', '-', 'forget', 'other_ids', 'retain', 'test']\n",
      "2025-02-07 12:34:35,-533679309 | INFO | 3841452 - Instantiating: erasure.data.datasets.DataSplitter.DataSplitterConcat\n",
      "2025-02-07 12:34:35,-533679307 | INFO | 3841452 - ['all', 'all_shuffled', '-', 'forget', 'other_ids', 'retain', 'test', 'train']\n",
      "2025-02-07 12:34:35,-533679306 | INFO | 3841452 - ['all', 'all_shuffled', '-', 'forget', 'other_ids', 'retain', 'test', 'train']\n",
      "2025-02-07 12:34:35,-533679306 | INFO | 3841452 - Created Configurable: erasure.data.datasets.DatasetManager.DatasetManager\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config_file = os.path.join(\"configs\", \"survey\", \"IMDB.jsonc\")\n",
    "#config_file = os.path.join(\"configs\", \"example.jsonc\")\n",
    "\n",
    "\n",
    "global_ctx = Global(config_file)\n",
    "global_ctx.factory = ConfigurableFactory(global_ctx)\n",
    "\n",
    "#Create Dataset\n",
    "data_manager = global_ctx.factory.get_object( Local( global_ctx.config.data ))\n",
    "global_ctx.dataset = data_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_manager.n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forget_loader, _ = data_manager.get_loader_for('forget')\n",
    "len(forget_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-07 12:35:24,-533630140 | INFO | 3841452 - TRAINING WITH 80193 samples\n"
     ]
    }
   ],
   "source": [
    "train_loader, _ = data_manager.get_loader_for('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "966"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forget = data_manager.partitions['forget']\n",
    "len(forget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 101, 2023, 2003,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 2023, 2143,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 2387, 2023,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 101, 1045, 1005,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 2010, 9427,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 1045, 2074,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]]])\n",
      "tensor([3, 5, 7, 8, 7, 8, 8, 4, 3, 8, 8, 0, 4, 8, 7, 1, 5, 7, 8, 8, 8, 7, 3, 1,\n",
      "        2, 8, 4, 2, 8, 4, 7, 6, 4, 8, 6, 8, 2, 8, 1, 5, 8, 8, 7, 2, 2, 5, 6, 8,\n",
      "        7, 1, 8, 8, 0, 8, 7, 8, 5, 4, 1, 8, 8, 8, 2, 8])\n",
      "tensor([[[  101,  2023,  3185,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  1045,  5993,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2009,  1005,  ..., 19234,  2015,   102],\n",
      "         [    1,     1,     1,  ...,     1,     1,     1]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  101,  1045,  2245,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2064,  2023,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  1000,  1996,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]]])\n",
      "tensor([7, 8, 8, 5, 0, 0, 5, 8, 8, 8, 8, 8, 7, 8, 3, 0, 8, 8, 0, 8, 1, 0, 8, 3,\n",
      "        4, 3, 8, 8, 7, 8, 7, 0, 8, 0, 8, 7, 5, 8, 8, 3, 7, 4, 7, 4, 8, 8, 5, 8,\n",
      "        8, 0, 8, 8, 3, 2, 8, 8, 8, 0, 8, 8, 3, 8, 0, 8])\n",
      "tensor([[[  101,  2023,  2318,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2023, 21459,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2096,  7440,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  101,  1045,  5223,  ...,  1007,   102,     0],\n",
      "         [    1,     1,     1,  ...,     1,     1,     0]],\n",
      "\n",
      "        [[  101,  2023,  2327,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2081,  2033,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]]])\n",
      "tensor([2, 8, 8, 6, 8, 2, 0, 8, 6, 8, 7, 7, 8, 8, 8, 4, 8, 6, 7, 5, 8, 8, 5, 7,\n",
      "        8, 8, 0, 8, 8, 8, 8, 0, 7, 0, 8, 4, 8, 8, 7, 4, 8, 3, 0, 8, 8, 8, 8, 3,\n",
      "        8, 8, 3, 8, 8, 8, 8, 3, 4, 4, 8, 8, 8, 8, 8, 8])\n",
      "tensor([[[  101, 16941,  4224,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  1996, 16434,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  1045,  2903,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  101,  1996,  2200,  ...,  5637,  3428,   102],\n",
      "         [    1,     1,     1,  ...,     1,     1,     1]],\n",
      "\n",
      "        [[  101,  2065,  2017,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2023,  2001,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]]])\n",
      "tensor([8, 8, 8, 8, 7, 8, 7, 8, 8, 1, 8, 8, 8, 1, 7, 8, 8, 5, 4, 7, 7, 5, 8, 8,\n",
      "        4, 0, 8, 3, 4, 0, 8, 8, 7, 8, 8, 2, 8, 4, 0, 8, 8, 8, 8, 8, 0, 8, 7, 8,\n",
      "        0, 7, 8, 8, 5, 8, 8, 8, 7, 6, 8, 8, 8, 7, 8, 8])\n",
      "tensor([[[  101,  2026,  2171,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  1008,  1008,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2023,  2001,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  101,  1996,  2307,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2153, 18566,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2004,  2017,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]]])\n",
      "tensor([4, 8, 8, 8, 8, 8, 4, 8, 3, 6, 8, 4, 8, 8, 8, 8, 7, 0, 8, 6, 2, 8, 8, 3,\n",
      "        4, 0, 8, 0, 2, 8, 8, 8, 7, 7, 1, 6, 8, 3, 3, 8, 6, 1, 5, 3, 4, 4, 8, 8,\n",
      "        8, 8, 0, 5, 7, 7, 8, 5, 0, 5, 8, 8, 8, 0, 6, 8])\n",
      "tensor([[[ 101, 1045, 3866,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 2348, 2023,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 2009, 1005,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 101, 1000, 1996,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 1045, 1005,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 1045, 3342,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]]])\n",
      "tensor([8, 5, 0, 8, 8, 8, 0, 3, 6, 5, 8, 5, 8, 2, 4, 4, 8, 8, 8, 8, 8, 8, 8, 2,\n",
      "        2, 8, 8, 7, 8, 8, 8, 8, 8, 6, 8, 2, 1, 8, 8, 2, 0, 8, 8, 0, 8, 4, 8, 8,\n",
      "        8, 8, 5, 8, 8, 8, 8, 3, 8, 8, 8, 8, 3, 8, 8, 8])\n",
      "tensor([[[ 101, 1000, 2023,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 5681, 6057,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 1037, 7376,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 101, 2750, 2070,  ..., 2023, 1010,  102],\n",
      "         [   1,    1,    1,  ...,    1,    1,    1]],\n",
      "\n",
      "        [[ 101, 2065, 2017,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 2005, 2035,  ..., 1028, 1026,  102],\n",
      "         [   1,    1,    1,  ...,    1,    1,    1]]])\n",
      "tensor([1, 3, 7, 8, 8, 8, 8, 3, 4, 2, 7, 7, 8, 7, 8, 7, 8, 4, 0, 7, 7, 4, 8, 7,\n",
      "        0, 0, 2, 8, 8, 5, 7, 0, 1, 2, 8, 8, 8, 8, 2, 8, 8, 5, 0, 7, 4, 1, 8, 0,\n",
      "        5, 8, 7, 5, 0, 6, 8, 7, 3, 8, 8, 8, 0, 8, 8, 8])\n",
      "tensor([[[  101,  2028,  1997,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  1045,  3685,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  4151, 10231,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  101,  1045,  2572,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2023,  3185,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  1996, 16074,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]]])\n",
      "tensor([8, 8, 8, 0, 3, 2, 6, 5, 8, 4, 6, 3, 2, 0, 6, 8, 5, 8, 8, 7, 8, 5, 3, 8,\n",
      "        6, 8, 4, 0, 6, 1, 8, 3, 5, 8, 1, 5, 8, 1, 8, 0, 8, 0, 8, 3, 6, 7, 8, 8,\n",
      "        7, 7, 7, 7, 0, 4, 0, 0, 4, 0, 8, 8, 8, 2, 4, 4])\n",
      "tensor([[[  101,  2823,  1045,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  1000,  2305,  ...,  2041,  2008,   102],\n",
      "         [    1,     1,     1,  ...,     1,     1,     1]],\n",
      "\n",
      "        [[  101,  2007,  2307,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  101,  1045,  2097,  ...,  2005, 10334,   102],\n",
      "         [    1,     1,     1,  ...,     1,     1,     1]],\n",
      "\n",
      "        [[  101, 10043,  2000,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2295,  2025,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]]])\n",
      "tensor([1, 5, 0, 8, 8, 8, 8, 8, 3, 1, 8, 8, 8, 0, 3, 8, 8, 6, 8, 8, 8, 7, 0, 0,\n",
      "        5, 0, 8, 0, 8, 8, 0, 8, 8, 8, 7, 5, 8, 5, 5, 0, 2, 6, 8, 5, 8, 8, 0, 8,\n",
      "        2, 8, 8, 4, 2, 1, 8, 6, 8, 8, 8, 7, 8, 8, 8, 7])\n",
      "tensor([[[  101,  1996,  2034,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  1043,  1005,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  1000,  1996,  ...,  1012,  1026,   102],\n",
      "         [    1,     1,     1,  ...,     1,     1,     1]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  101, 22249,  2210,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2023,  2001,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101, 19817, 10354,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]]])\n",
      "tensor([8, 8, 0, 8, 0, 2, 8, 8, 8, 5, 8, 0, 7, 8, 0, 7, 5, 2, 8, 8, 0, 0, 8, 7,\n",
      "        8, 8, 8, 8, 8, 8, 8, 7, 2, 8, 8, 8, 8, 0, 6, 5, 0, 2, 8, 8, 8, 8, 5, 0,\n",
      "        8, 7, 8, 0, 6, 7, 7, 0, 1, 8, 8, 2, 6, 8, 0, 7])\n",
      "tensor([[[  101,  2061,  2054,  ...,  2004, 10271,   102],\n",
      "         [    1,     1,     1,  ...,     1,     1,     1]],\n",
      "\n",
      "        [[  101,  2158,  2821,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2348,  6052,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  101,  2009,  1005,  ...,  6574,  1997,   102],\n",
      "         [    1,     1,     1,  ...,     1,     1,     1]],\n",
      "\n",
      "        [[  101,  2023,  2003,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  1045,  2123,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]]])\n",
      "tensor([7, 2, 8, 8, 5, 7, 3, 8, 8, 8, 7, 8, 7, 7, 7, 8, 8, 7, 3, 7, 1, 8, 8, 4,\n",
      "        8, 8, 3, 8, 8, 0, 8, 0, 7, 2, 8, 7, 7, 8, 1, 8, 8, 3, 4, 8, 0, 8, 8, 8,\n",
      "        1, 5, 8, 8, 8, 6, 6, 8, 6, 8, 5, 7, 8, 6, 2, 7])\n",
      "tensor([[[ 101, 1045, 3427,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 2348, 2045,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 1045, 2031,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 101, 1996, 2143,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 1045, 2034,  ..., 1005, 2397,  102],\n",
      "         [   1,    1,    1,  ...,    1,    1,    1]],\n",
      "\n",
      "        [[ 101, 4067, 2643,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]]])\n",
      "tensor([8, 7, 8, 8, 8, 0, 8, 0, 8, 2, 8, 7, 8, 0, 5, 7, 8, 8, 8, 6, 7, 4, 3, 3,\n",
      "        2, 8, 7, 0, 2, 5, 4, 0, 6, 3, 8, 0, 7, 5, 6, 3, 4, 2, 7, 8, 7, 2, 5, 8,\n",
      "        2, 8, 4, 8, 0, 8, 1, 8, 8, 7, 1, 3, 8, 8, 8, 0])\n",
      "tensor([[[ 101, 1045, 9826,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 2009, 1005,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 2035, 1045,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 101, 2023, 2003,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 1045, 4622,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 1043, 7100,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]]])\n",
      "tensor([8, 8, 7, 8, 8, 8, 8, 6, 8, 8, 4, 8, 3, 1, 8, 3, 8, 8, 8, 8, 7, 1, 6, 7,\n",
      "        1, 8, 5, 5, 0, 8, 8, 1, 3, 8, 5, 8, 8, 8, 0, 1, 8, 8, 8, 8, 4, 6, 0, 3,\n",
      "        7, 1, 8, 0, 8, 0, 8, 8, 5, 3, 3, 3, 2, 6, 8, 7])\n",
      "tensor([[[  101,  3100,  1010,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2026,  6513,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2065,  2017,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  101,  2166,  1005,  ..., 16010,  2477,   102],\n",
      "         [    1,     1,     1,  ...,     1,     1,     1]],\n",
      "\n",
      "        [[  101,  1045,  2941,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2750,  1996,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]]])\n",
      "tensor([0, 8, 8, 8, 1, 8, 0, 3, 7, 8, 8, 8, 8, 1, 8, 8, 1, 2, 8, 5, 3, 4, 0, 3,\n",
      "        8, 8, 8, 0, 8, 6, 8, 2, 4, 6, 8, 8, 3, 8, 8, 8, 4, 5, 1, 6, 6, 4, 2, 8,\n",
      "        8, 5, 8, 8, 1, 8, 3, 7, 8, 8, 7, 4, 8, 5, 8, 8])\n",
      "tensor([[[ 101, 1996, 2711,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 1996, 2087,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 2619, 3191,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 101, 1037, 9202,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 1045, 1005,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 1045, 2031,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]]])\n",
      "tensor([8, 5, 8, 0, 2, 8, 8, 8, 8, 7, 5, 1, 8, 5, 8, 8, 0, 8, 8, 8, 7, 7, 0, 5,\n",
      "        8, 5, 8, 8, 8, 8, 2, 8, 7, 3, 6, 8, 8, 8, 5, 1, 8, 8, 0, 7, 2, 2, 3, 0,\n",
      "        3, 6, 7, 0, 0, 8, 8, 2, 8, 3, 8, 6, 7, 8, 6, 0])\n",
      "tensor([[[  101,  2023,  2460,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2023, 25628,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  1045,  2179,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  101,  2026,  2202,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  3053,  2673,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2023,  2001,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]]])\n",
      "tensor([8, 2, 5, 8, 1, 8, 1, 8, 8, 8, 8, 8, 0, 8, 2, 8, 0, 4, 8, 8, 8, 7, 8, 8,\n",
      "        7, 0, 7, 8, 8, 8, 3, 8, 6, 7, 8, 7, 4, 8, 0, 7, 5, 0, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 7, 5, 7, 6, 4, 8, 7, 8, 8, 8, 8, 4, 3, 5])\n",
      "tensor([[[  101,  1000,  2167,  ...,  1005, 29554,   102],\n",
      "         [    1,     1,     1,  ...,     1,     1,     1]],\n",
      "\n",
      "        [[  101,  2763,  4445,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2023,  2143,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  101,  1000, 15333,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  1037,  2767,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  1045,  2064,  ...,  2002,  4669,   102],\n",
      "         [    1,     1,     1,  ...,     1,     1,     1]]])\n",
      "tensor([7, 8, 3, 8, 8, 4, 8, 8, 5, 0, 5, 8, 7, 6, 5, 7, 8, 8, 7, 8, 3, 8, 8, 8,\n",
      "        5, 8, 4, 7, 8, 8, 8, 7, 8, 4, 8, 8, 7, 8, 8, 8, 8, 8, 5, 5, 6, 5, 8, 2,\n",
      "        8, 0, 8, 3, 3, 8, 8, 3, 8, 8, 0, 2, 8, 8, 8, 0])\n",
      "tensor([[[ 101, 1045, 2001,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 1037, 2158,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 1045, 6758,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 101, 2023, 2003,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 2023, 2003,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 1045, 3866,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]]])\n",
      "tensor([8, 8, 4, 8, 2, 8, 8, 8, 8, 7, 8, 8, 7, 0, 3, 8, 7, 8, 8, 8, 4, 8, 0, 8,\n",
      "        8, 8, 0, 8, 4, 8, 8, 5, 3, 2, 1, 2, 0, 1, 8, 1, 8, 8, 3, 8, 8, 4, 5, 4,\n",
      "        8, 8, 8, 0, 8, 8, 8, 1, 4, 8, 7, 8, 0, 7, 4, 7])\n",
      "tensor([[[  101,  2023,  2003,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  1996,  4462,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  1045,  2387,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  101,  1045,  8343,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2045,  1005,  ...,  2028,  2060,   102],\n",
      "         [    1,     1,     1,  ...,     1,     1,     1]],\n",
      "\n",
      "        [[  101,  2120, 10437,  ...,     0,     0,     0],\n",
      "         [    1,     1,     1,  ...,     0,     0,     0]]])\n",
      "tensor([8, 8, 8, 5, 5, 6, 3, 0, 6, 8, 1, 8, 5, 8, 3, 8, 3, 4, 8, 5, 0, 8, 8, 0,\n",
      "        8, 8, 8, 5, 0, 8, 4, 8, 1, 8, 0, 8, 8, 8, 5, 8, 8, 8, 8, 5, 7, 1, 8, 8,\n",
      "        8, 8, 8, 8, 4, 1, 8, 2, 8, 8, 8, 3, 8, 8, 3, 0])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Iterate through your DataLoader\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(X)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(y)\n",
      "File \u001b[0;32m~/miniconda3/envs/representer/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/representer/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/representer/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/representer/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/ERASURE/erasure/data/datasets/Dataset.py:18\u001b[0m, in \u001b[0;36mDatasetWrapper.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m     17\u001b[0m     X,y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__realgetitem__(index)\n\u001b[0;32m---> 18\u001b[0m     X,y,Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X,y\n",
      "File \u001b[0;32m~/ERASURE/erasure/data/datasets/Dataset.py:32\u001b[0m, in \u001b[0;36mDatasetWrapper.apply_preprocessing\u001b[0;34m(self, X, y, Z)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03mApply each preprocessing step to the data (X, y).\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m preprocess \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess:\n\u001b[0;32m---> 32\u001b[0m     X,y,Z \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, Z\n",
      "File \u001b[0;32m~/ERASURE/erasure/data/preprocessing/tokenize_text.py:22\u001b[0m, in \u001b[0;36mTokenizeX.process\u001b[0;34m(self, X, y, z)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, z):    \n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mstr\u001b[39m): \n\u001b[0;32m---> 22\u001b[0m         tokenized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m         X \u001b[38;5;241m=\u001b[39m tokenized[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)  \n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/ERASURE/erasure/data/preprocessing/tokenize_text.py:13\u001b[0m, in \u001b[0;36mTokenizerWrapper.__call__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/representer/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2868\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2866\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2867\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2868\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2870\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/envs/representer/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2978\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2956\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2957\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2958\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2975\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2976\u001b[0m     )\n\u001b[1;32m   2977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2981\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2997\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2998\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/representer/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3054\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3045\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3046\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3047\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3051\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3052\u001b[0m )\n\u001b[0;32m-> 3054\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3057\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3073\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3074\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3075\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/representer/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:613\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    591\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    611\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    612\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 613\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/miniconda3/envs/representer/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:539\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m!=\u001b[39m split_special_tokens:\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m=\u001b[39m split_special_tokens\n\u001b[0;32m--> 539\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    551\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    553\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    563\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the same tokenizer used for tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Iterate through your DataLoader\n",
    "for X, y in train_loader:\n",
    "    print(X)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m))  \u001b[38;5;66;03m# Set the figure size\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m forget_loader:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Convert the batch to NumPy\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     np_images \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Change shape to (N, H, W, C)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Loop through images in the batch\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(np_images, y):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "num_images_to_display = 40\n",
    "images_per_row = 4  # Number of images per row for a grid layout\n",
    "images_displayed = 0\n",
    "\n",
    "# Collect and display images\n",
    "plt.figure(figsize=(15, 10))  # Set the figure size\n",
    "\n",
    "for X, y in forget_loader:\n",
    "    # Convert the batch to NumPy\n",
    "    np_images = X.permute(0, 2, 3, 1).numpy()  # Change shape to (N, H, W, C)\n",
    "    \n",
    "    # Loop through images in the batch\n",
    "    for img, label in zip(np_images, y):\n",
    "        images_displayed += 1\n",
    "        \n",
    "        # Add subplot for each image\n",
    "        plt.subplot(num_images_to_display // images_per_row, images_per_row, images_displayed)\n",
    "        plt.imshow(img)  # Assuming the range is already suitable for visualization\n",
    "        plt.title(f\"Label: {label.item()}\")  # Display label\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        if images_displayed == num_images_to_display:\n",
    "            break  # Stop after displaying the required number of images\n",
    "    if images_displayed == num_images_to_display:\n",
    "        break\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing between subplots\n",
    "plt.savefig('2820_new_new.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-31 17:09:43,-1121971173 | INFO | 346018 - TRAINING WITH 124594 samples\n"
     ]
    }
   ],
   "source": [
    "loader, _ = data_manager.get_loader_for('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-06 23:09:49,-581965562 | INFO | 3533630 - Instantiating: erasure.model.classifiers.BERT_classifier.BERTClassifier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-06 23:09:50,-581964306 | INFO | 3533630 - Instantiating: torch.optim.AdamW\n",
      "2025-02-06 23:09:50,-581964301 | INFO | 3533630 - Instantiating: torch.nn.CrossEntropyLoss\n",
      "2025-02-06 23:09:50,-581964278 | INFO | 3533630 - TRAINING WITH 102142 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from erasure.utils.config.global_ctx import bcolors\n",
    "\n",
    "\n",
    "#Create Predictor\n",
    "current = Local(global_ctx.config.predictor)\n",
    "current.dataset = data_manager\n",
    "predictor = global_ctx.factory.get_object(current)\n",
    "global_ctx.predictor = predictor\n",
    "global_ctx.logger.info('Global Predictor: ' + str(predictor))\n",
    "\n",
    "#Create unlearners \n",
    "unlearners = []\n",
    "unlearners_cfg = global_ctx.config.unlearners\n",
    "for un in unlearners_cfg:\n",
    "    current = Local(un)\n",
    "    current.dataset = data_manager\n",
    "    current.predictor = copy.deepcopy(predictor)\n",
    "    unlearners.append( global_ctx.factory.get_object(current) )\n",
    "\n",
    "\n",
    "#Evaluator\n",
    "current = Local(global_ctx.config.evaluator)\n",
    "current.unlearners = unlearners\n",
    "evaluator = global_ctx.factory.get_object(current)\n",
    "\n",
    "# Evaluations\n",
    "for unlearner in unlearners:\n",
    "    global_ctx.logger.info(f'''{bcolors.OKGREEN}####\\t\\t Evaluating: {unlearner.__class__.__name__} \\t\\t####{bcolors.ENDC}''')\n",
    "    evaluator.evaluate(unlearner,predictor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "representer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
