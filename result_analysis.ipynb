{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load json file \n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset_name = \"celeba_20_ids\"\n",
    "\n",
    "paths = [f\"output/runs/survey/{dataset_name}.json\", f\"output/runs/survey/{dataset_name}_seed_1.json\", f\"output/runs/survey/{dataset_name}_seed_1.json\"]\n",
    "\n",
    "datasets = []\n",
    "for path in paths: \n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    len(data)\n",
    "\n",
    "    for entry in data:\n",
    "        try :\n",
    "            entry['lr'] = entry['parameters']['optimizer']['parameters']['lr']\n",
    "        except KeyError:\n",
    "            entry['lr'] = None\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    # remove dataset column \n",
    "    df = df.drop(columns=['dataset', 'parameters', 'RunTime', 'sklearn.metrics.accuracy_score.test.original', 'sklearn.metrics.accuracy_score.forget.original', 'sklearn.metrics.accuracy_score.retain.original'])\n",
    "\n",
    "    forget_accuracies = df['sklearn.metrics.accuracy_score.forget.unlearned']\n",
    "    gold_accuracy = df['sklearn.metrics.accuracy_score.forget.unlearned'][0]\n",
    "    df['forget_diff'] = np.abs(forget_accuracies - gold_accuracy)\n",
    "\n",
    "    def nomus(f1, forget_score, l=0.5):\n",
    "        return l * f1 + (1 - l) * (1-forget_score*2)\n",
    "    \n",
    "    df['Unlearning_Score'] = np.abs(df['UMIA'] - 0.5)\n",
    "\n",
    "    df['nomus'] = nomus(df['sklearn.metrics.accuracy_score.test.unlearned'], df['Unlearning_Score'])\n",
    "\n",
    "    # Remove relearning score \n",
    "    df = df.drop(columns=['RelearnTime'])\n",
    "    \n",
    "\n",
    "    \n",
    "    datasets.append(df)\n",
    "\n",
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 36, 36)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets[0]), len(datasets[1]), len(datasets[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unlearner</th>\n",
       "      <th>sklearn.metrics.accuracy_score.test.unlearned</th>\n",
       "      <th>sklearn.metrics.accuracy_score.forget.unlearned</th>\n",
       "      <th>sklearn.metrics.accuracy_score.retain.unlearned</th>\n",
       "      <th>AUS</th>\n",
       "      <th>AIN</th>\n",
       "      <th>UMIA</th>\n",
       "      <th>lr</th>\n",
       "      <th>forget_diff</th>\n",
       "      <th>Unlearning_Score</th>\n",
       "      <th>nomus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GoldModel</td>\n",
       "      <td>0.923653</td>\n",
       "      <td>0.927374</td>\n",
       "      <td>0.938075</td>\n",
       "      <td>0.993613</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.508837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008837</td>\n",
       "      <td>0.952989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdvancedNegGrad</td>\n",
       "      <td>0.868794</td>\n",
       "      <td>0.031657</td>\n",
       "      <td>0.892745</td>\n",
       "      <td>0.513000</td>\n",
       "      <td>1.995025</td>\n",
       "      <td>0.966977</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.895717</td>\n",
       "      <td>0.466977</td>\n",
       "      <td>0.467420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdvancedNegGrad</td>\n",
       "      <td>0.695875</td>\n",
       "      <td>0.402235</td>\n",
       "      <td>0.710111</td>\n",
       "      <td>0.594858</td>\n",
       "      <td>49.756219</td>\n",
       "      <td>0.662791</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.525140</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.685147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BadTeaching</td>\n",
       "      <td>0.519443</td>\n",
       "      <td>0.536313</td>\n",
       "      <td>0.517639</td>\n",
       "      <td>0.583262</td>\n",
       "      <td>1.995025</td>\n",
       "      <td>0.502326</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.391061</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.757396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BadTeaching</td>\n",
       "      <td>0.480557</td>\n",
       "      <td>0.463687</td>\n",
       "      <td>0.482361</td>\n",
       "      <td>0.545020</td>\n",
       "      <td>1.995025</td>\n",
       "      <td>0.490930</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.463687</td>\n",
       "      <td>0.009070</td>\n",
       "      <td>0.731209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BadTeaching</td>\n",
       "      <td>0.525810</td>\n",
       "      <td>0.545624</td>\n",
       "      <td>0.523759</td>\n",
       "      <td>0.587821</td>\n",
       "      <td>13.935323</td>\n",
       "      <td>0.518372</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.381750</td>\n",
       "      <td>0.018372</td>\n",
       "      <td>0.744533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Finetuning</td>\n",
       "      <td>0.924037</td>\n",
       "      <td>0.949721</td>\n",
       "      <td>0.939104</td>\n",
       "      <td>0.972712</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.528372</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.022346</td>\n",
       "      <td>0.028372</td>\n",
       "      <td>0.933647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Finetuning</td>\n",
       "      <td>0.923406</td>\n",
       "      <td>0.949721</td>\n",
       "      <td>0.948626</td>\n",
       "      <td>0.971499</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.526279</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.022346</td>\n",
       "      <td>0.026279</td>\n",
       "      <td>0.935424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Finetuning</td>\n",
       "      <td>0.928675</td>\n",
       "      <td>0.968343</td>\n",
       "      <td>0.944400</td>\n",
       "      <td>0.964090</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.534419</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.040968</td>\n",
       "      <td>0.034419</td>\n",
       "      <td>0.929919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Finetuning</td>\n",
       "      <td>0.926315</td>\n",
       "      <td>0.957169</td>\n",
       "      <td>0.939474</td>\n",
       "      <td>0.970043</td>\n",
       "      <td>7.467662</td>\n",
       "      <td>0.537209</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.029795</td>\n",
       "      <td>0.037209</td>\n",
       "      <td>0.925948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Finetuning</td>\n",
       "      <td>0.926452</td>\n",
       "      <td>0.955307</td>\n",
       "      <td>0.939310</td>\n",
       "      <td>0.972061</td>\n",
       "      <td>13.935323</td>\n",
       "      <td>0.542326</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.027933</td>\n",
       "      <td>0.042326</td>\n",
       "      <td>0.920901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Finetuning</td>\n",
       "      <td>0.926370</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.939021</td>\n",
       "      <td>0.966655</td>\n",
       "      <td>17.417910</td>\n",
       "      <td>0.537442</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.037442</td>\n",
       "      <td>0.925743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FisherForgetting</td>\n",
       "      <td>0.900272</td>\n",
       "      <td>0.923650</td>\n",
       "      <td>0.908374</td>\n",
       "      <td>0.951680</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.540698</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>0.040698</td>\n",
       "      <td>0.909438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FisherForgetting</td>\n",
       "      <td>0.908395</td>\n",
       "      <td>0.947858</td>\n",
       "      <td>0.916902</td>\n",
       "      <td>0.944768</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.542558</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020484</td>\n",
       "      <td>0.042558</td>\n",
       "      <td>0.911639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FisherForgetting</td>\n",
       "      <td>0.881940</td>\n",
       "      <td>0.919926</td>\n",
       "      <td>0.890790</td>\n",
       "      <td>0.920626</td>\n",
       "      <td>0.502488</td>\n",
       "      <td>0.532326</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007449</td>\n",
       "      <td>0.032326</td>\n",
       "      <td>0.908644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>eu_k</td>\n",
       "      <td>0.926452</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.939618</td>\n",
       "      <td>0.966811</td>\n",
       "      <td>1.497512</td>\n",
       "      <td>0.532791</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.032791</td>\n",
       "      <td>0.930436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Identity</td>\n",
       "      <td>0.926343</td>\n",
       "      <td>0.962756</td>\n",
       "      <td>0.938946</td>\n",
       "      <td>0.964866</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.535116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.035382</td>\n",
       "      <td>0.035116</td>\n",
       "      <td>0.928055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NegGrad</td>\n",
       "      <td>0.926233</td>\n",
       "      <td>0.962756</td>\n",
       "      <td>0.938953</td>\n",
       "      <td>0.964658</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.533953</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.035382</td>\n",
       "      <td>0.033953</td>\n",
       "      <td>0.929163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NegGrad</td>\n",
       "      <td>0.927166</td>\n",
       "      <td>0.955307</td>\n",
       "      <td>0.939399</td>\n",
       "      <td>0.973430</td>\n",
       "      <td>2.492537</td>\n",
       "      <td>0.538837</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.027933</td>\n",
       "      <td>0.038837</td>\n",
       "      <td>0.924746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NegGrad</td>\n",
       "      <td>0.726474</td>\n",
       "      <td>0.709497</td>\n",
       "      <td>0.729918</td>\n",
       "      <td>0.786775</td>\n",
       "      <td>1.995025</td>\n",
       "      <td>0.498605</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.217877</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.861842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Scrub</td>\n",
       "      <td>0.925958</td>\n",
       "      <td>0.953445</td>\n",
       "      <td>0.947295</td>\n",
       "      <td>0.972875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.535349</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.026071</td>\n",
       "      <td>0.035349</td>\n",
       "      <td>0.927630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Scrub</td>\n",
       "      <td>0.911743</td>\n",
       "      <td>0.944134</td>\n",
       "      <td>0.920456</td>\n",
       "      <td>0.954483</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.530930</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>0.030930</td>\n",
       "      <td>0.924941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Scrub</td>\n",
       "      <td>0.519443</td>\n",
       "      <td>0.536313</td>\n",
       "      <td>0.517639</td>\n",
       "      <td>0.583262</td>\n",
       "      <td>10.452736</td>\n",
       "      <td>0.509302</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.391061</td>\n",
       "      <td>0.009302</td>\n",
       "      <td>0.750419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SelectiveSynapticDampening</td>\n",
       "      <td>0.925492</td>\n",
       "      <td>0.966480</td>\n",
       "      <td>0.938335</td>\n",
       "      <td>0.959808</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.545814</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.039106</td>\n",
       "      <td>0.045814</td>\n",
       "      <td>0.916932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SelectiveSynapticDampening</td>\n",
       "      <td>0.925492</td>\n",
       "      <td>0.966480</td>\n",
       "      <td>0.938335</td>\n",
       "      <td>0.959808</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.547442</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.039106</td>\n",
       "      <td>0.047442</td>\n",
       "      <td>0.915304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SelectiveSynapticDampening</td>\n",
       "      <td>0.925464</td>\n",
       "      <td>0.966480</td>\n",
       "      <td>0.938335</td>\n",
       "      <td>0.959756</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.540930</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.039106</td>\n",
       "      <td>0.040930</td>\n",
       "      <td>0.921802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SelectiveSynapticDampening</td>\n",
       "      <td>0.925464</td>\n",
       "      <td>0.966480</td>\n",
       "      <td>0.938335</td>\n",
       "      <td>0.959756</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.544186</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.039106</td>\n",
       "      <td>0.044186</td>\n",
       "      <td>0.918546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Cascade</td>\n",
       "      <td>0.924888</td>\n",
       "      <td>0.945996</td>\n",
       "      <td>0.928367</td>\n",
       "      <td>0.977904</td>\n",
       "      <td>1.497512</td>\n",
       "      <td>0.512326</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018622</td>\n",
       "      <td>0.012326</td>\n",
       "      <td>0.950119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Cascade</td>\n",
       "      <td>0.924257</td>\n",
       "      <td>0.931099</td>\n",
       "      <td>0.926199</td>\n",
       "      <td>0.991133</td>\n",
       "      <td>1.497512</td>\n",
       "      <td>0.518372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>0.018372</td>\n",
       "      <td>0.943756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Cascade</td>\n",
       "      <td>0.923434</td>\n",
       "      <td>0.936685</td>\n",
       "      <td>0.927475</td>\n",
       "      <td>0.984051</td>\n",
       "      <td>1.497512</td>\n",
       "      <td>0.519767</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009311</td>\n",
       "      <td>0.019767</td>\n",
       "      <td>0.941949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>SuccessiveRandomLabels</td>\n",
       "      <td>0.922199</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.924984</td>\n",
       "      <td>0.979837</td>\n",
       "      <td>1.497512</td>\n",
       "      <td>0.505349</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.005349</td>\n",
       "      <td>0.955751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>SuccessiveRandomLabels</td>\n",
       "      <td>0.925163</td>\n",
       "      <td>0.947858</td>\n",
       "      <td>0.939934</td>\n",
       "      <td>0.976654</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.527907</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.020484</td>\n",
       "      <td>0.027907</td>\n",
       "      <td>0.934674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>SuccessiveRandomLabels</td>\n",
       "      <td>0.923653</td>\n",
       "      <td>0.949721</td>\n",
       "      <td>0.948503</td>\n",
       "      <td>0.971974</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.515349</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.022346</td>\n",
       "      <td>0.015349</td>\n",
       "      <td>0.946478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Cascade</td>\n",
       "      <td>0.923434</td>\n",
       "      <td>0.947858</td>\n",
       "      <td>0.939159</td>\n",
       "      <td>0.973318</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.519302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020484</td>\n",
       "      <td>0.019302</td>\n",
       "      <td>0.942415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Cascade</td>\n",
       "      <td>0.920442</td>\n",
       "      <td>0.934823</td>\n",
       "      <td>0.922082</td>\n",
       "      <td>0.980007</td>\n",
       "      <td>3.985075</td>\n",
       "      <td>0.516279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007449</td>\n",
       "      <td>0.016279</td>\n",
       "      <td>0.943942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Cascade</td>\n",
       "      <td>0.924641</td>\n",
       "      <td>0.949721</td>\n",
       "      <td>0.948469</td>\n",
       "      <td>0.973874</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.525116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022346</td>\n",
       "      <td>0.025116</td>\n",
       "      <td>0.937204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     unlearner  sklearn.metrics.accuracy_score.test.unlearned  \\\n",
       "0                    GoldModel                                       0.923653   \n",
       "1              AdvancedNegGrad                                       0.868794   \n",
       "2              AdvancedNegGrad                                       0.695875   \n",
       "3                  BadTeaching                                       0.519443   \n",
       "4                  BadTeaching                                       0.480557   \n",
       "5                  BadTeaching                                       0.525810   \n",
       "6                   Finetuning                                       0.924037   \n",
       "7                   Finetuning                                       0.923406   \n",
       "8                   Finetuning                                       0.928675   \n",
       "9                   Finetuning                                       0.926315   \n",
       "10                  Finetuning                                       0.926452   \n",
       "11                  Finetuning                                       0.926370   \n",
       "12            FisherForgetting                                       0.900272   \n",
       "13            FisherForgetting                                       0.908395   \n",
       "14            FisherForgetting                                       0.881940   \n",
       "15                        eu_k                                       0.926452   \n",
       "16                    Identity                                       0.926343   \n",
       "17                     NegGrad                                       0.926233   \n",
       "18                     NegGrad                                       0.927166   \n",
       "19                     NegGrad                                       0.726474   \n",
       "20                       Scrub                                       0.925958   \n",
       "21                       Scrub                                       0.911743   \n",
       "22                       Scrub                                       0.519443   \n",
       "23  SelectiveSynapticDampening                                       0.925492   \n",
       "24  SelectiveSynapticDampening                                       0.925492   \n",
       "25  SelectiveSynapticDampening                                       0.925464   \n",
       "26  SelectiveSynapticDampening                                       0.925464   \n",
       "27                     Cascade                                       0.924888   \n",
       "28                     Cascade                                       0.924257   \n",
       "29                     Cascade                                       0.923434   \n",
       "30      SuccessiveRandomLabels                                       0.922199   \n",
       "31      SuccessiveRandomLabels                                       0.925163   \n",
       "32      SuccessiveRandomLabels                                       0.923653   \n",
       "33                     Cascade                                       0.923434   \n",
       "34                     Cascade                                       0.920442   \n",
       "35                     Cascade                                       0.924641   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.forget.unlearned  \\\n",
       "0                                          0.927374   \n",
       "1                                          0.031657   \n",
       "2                                          0.402235   \n",
       "3                                          0.536313   \n",
       "4                                          0.463687   \n",
       "5                                          0.545624   \n",
       "6                                          0.949721   \n",
       "7                                          0.949721   \n",
       "8                                          0.968343   \n",
       "9                                          0.957169   \n",
       "10                                         0.955307   \n",
       "11                                         0.960894   \n",
       "12                                         0.923650   \n",
       "13                                         0.947858   \n",
       "14                                         0.919926   \n",
       "15                                         0.960894   \n",
       "16                                         0.962756   \n",
       "17                                         0.962756   \n",
       "18                                         0.955307   \n",
       "19                                         0.709497   \n",
       "20                                         0.953445   \n",
       "21                                         0.944134   \n",
       "22                                         0.536313   \n",
       "23                                         0.966480   \n",
       "24                                         0.966480   \n",
       "25                                         0.966480   \n",
       "26                                         0.966480   \n",
       "27                                         0.945996   \n",
       "28                                         0.931099   \n",
       "29                                         0.936685   \n",
       "30                                         0.938547   \n",
       "31                                         0.947858   \n",
       "32                                         0.949721   \n",
       "33                                         0.947858   \n",
       "34                                         0.934823   \n",
       "35                                         0.949721   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.retain.unlearned       AUS        AIN  \\\n",
       "0                                          0.938075  0.993613   1.000000   \n",
       "1                                          0.892745  0.513000   1.995025   \n",
       "2                                          0.710111  0.594858  49.756219   \n",
       "3                                          0.517639  0.583262   1.995025   \n",
       "4                                          0.482361  0.545020   1.995025   \n",
       "5                                          0.523759  0.587821  13.935323   \n",
       "6                                          0.939104  0.972712   1.000000   \n",
       "7                                          0.948626  0.971499   1.000000   \n",
       "8                                          0.944400  0.964090   0.004975   \n",
       "9                                          0.939474  0.970043   7.467662   \n",
       "10                                         0.939310  0.972061  13.935323   \n",
       "11                                         0.939021  0.966655  17.417910   \n",
       "12                                         0.908374  0.951680   1.000000   \n",
       "13                                         0.916902  0.944768   1.000000   \n",
       "14                                         0.890790  0.920626   0.502488   \n",
       "15                                         0.939618  0.966811   1.497512   \n",
       "16                                         0.938946  0.964866   0.004975   \n",
       "17                                         0.938953  0.964658   0.004975   \n",
       "18                                         0.939399  0.973430   2.492537   \n",
       "19                                         0.729918  0.786775   1.995025   \n",
       "20                                         0.947295  0.972875   1.000000   \n",
       "21                                         0.920456  0.954483   1.000000   \n",
       "22                                         0.517639  0.583262  10.452736   \n",
       "23                                         0.938335  0.959808   0.004975   \n",
       "24                                         0.938335  0.959808   0.004975   \n",
       "25                                         0.938335  0.959756   0.004975   \n",
       "26                                         0.938335  0.959756   0.004975   \n",
       "27                                         0.928367  0.977904   1.497512   \n",
       "28                                         0.926199  0.991133   1.497512   \n",
       "29                                         0.927475  0.984051   1.497512   \n",
       "30                                         0.924984  0.979837   1.497512   \n",
       "31                                         0.939934  0.976654   1.000000   \n",
       "32                                         0.948503  0.971974   1.000000   \n",
       "33                                         0.939159  0.973318   1.000000   \n",
       "34                                         0.922082  0.980007   3.985075   \n",
       "35                                         0.948469  0.973874   1.000000   \n",
       "\n",
       "        UMIA       lr  forget_diff  Unlearning_Score     nomus  \n",
       "0   0.508837      NaN     0.000000          0.008837  0.952989  \n",
       "1   0.966977  0.00010     0.895717          0.466977  0.467420  \n",
       "2   0.662791  0.00001     0.525140          0.162791  0.685147  \n",
       "3   0.502326  0.00100     0.391061          0.002326  0.757396  \n",
       "4   0.490930  0.00010     0.463687          0.009070  0.731209  \n",
       "5   0.518372  0.00001     0.381750          0.018372  0.744533  \n",
       "6   0.528372  0.00100     0.022346          0.028372  0.933647  \n",
       "7   0.526279  0.00010     0.022346          0.026279  0.935424  \n",
       "8   0.534419  0.00001     0.040968          0.034419  0.929919  \n",
       "9   0.537209  0.00100     0.029795          0.037209  0.925948  \n",
       "10  0.542326  0.00010     0.027933          0.042326  0.920901  \n",
       "11  0.537442  0.00001     0.033520          0.037442  0.925743  \n",
       "12  0.540698      NaN     0.003724          0.040698  0.909438  \n",
       "13  0.542558      NaN     0.020484          0.042558  0.911639  \n",
       "14  0.532326      NaN     0.007449          0.032326  0.908644  \n",
       "15  0.532791  0.01000     0.033520          0.032791  0.930436  \n",
       "16  0.535116      NaN     0.035382          0.035116  0.928055  \n",
       "17  0.533953  0.00001     0.035382          0.033953  0.929163  \n",
       "18  0.538837  0.00010     0.027933          0.038837  0.924746  \n",
       "19  0.498605  0.00100     0.217877          0.001395  0.861842  \n",
       "20  0.535349  0.00010     0.026071          0.035349  0.927630  \n",
       "21  0.530930  0.00100     0.016760          0.030930  0.924941  \n",
       "22  0.509302  0.01000     0.391061          0.009302  0.750419  \n",
       "23  0.545814  0.01000     0.039106          0.045814  0.916932  \n",
       "24  0.547442  0.01000     0.039106          0.047442  0.915304  \n",
       "25  0.540930  0.01000     0.039106          0.040930  0.921802  \n",
       "26  0.544186  0.01000     0.039106          0.044186  0.918546  \n",
       "27  0.512326      NaN     0.018622          0.012326  0.950119  \n",
       "28  0.518372      NaN     0.003724          0.018372  0.943756  \n",
       "29  0.519767      NaN     0.009311          0.019767  0.941949  \n",
       "30  0.505349  0.01000     0.011173          0.005349  0.955751  \n",
       "31  0.527907  0.00100     0.020484          0.027907  0.934674  \n",
       "32  0.515349  0.00010     0.022346          0.015349  0.946478  \n",
       "33  0.519302      NaN     0.020484          0.019302  0.942415  \n",
       "34  0.516279      NaN     0.007449          0.016279  0.943942  \n",
       "35  0.525116      NaN     0.022346          0.025116  0.937204  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unlearner', 'sklearn.metrics.accuracy_score.test.unlearned',\n",
       "       'sklearn.metrics.accuracy_score.forget.unlearned',\n",
       "       'sklearn.metrics.accuracy_score.retain.unlearned', 'AUS', 'AIN', 'UMIA',\n",
       "       'lr', 'forget_diff', 'Unlearning_Score', 'nomus'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = datasets[0].columns\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unlearner</th>\n",
       "      <th>sklearn.metrics.accuracy_score.test.unlearned</th>\n",
       "      <th>sklearn.metrics.accuracy_score.forget.unlearned</th>\n",
       "      <th>sklearn.metrics.accuracy_score.retain.unlearned</th>\n",
       "      <th>AUS</th>\n",
       "      <th>AIN</th>\n",
       "      <th>UMIA</th>\n",
       "      <th>lr</th>\n",
       "      <th>forget_diff</th>\n",
       "      <th>Unlearning_Score</th>\n",
       "      <th>nomus</th>\n",
       "      <th>sklearn.metrics.accuracy_score.test.unlearned_std</th>\n",
       "      <th>sklearn.metrics.accuracy_score.forget.unlearned_std</th>\n",
       "      <th>sklearn.metrics.accuracy_score.retain.unlearned_std</th>\n",
       "      <th>AUS_std</th>\n",
       "      <th>AIN_std</th>\n",
       "      <th>UMIA_std</th>\n",
       "      <th>forget_diff_std</th>\n",
       "      <th>Unlearning_Score_std</th>\n",
       "      <th>nomus_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GoldModel</td>\n",
       "      <td>0.924166</td>\n",
       "      <td>0.936065</td>\n",
       "      <td>0.937864</td>\n",
       "      <td>0.989631</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>0.508837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008837</td>\n",
       "      <td>0.953246</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>1.487733e-04</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>0.156613</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdvancedNegGrad</td>\n",
       "      <td>0.888901</td>\n",
       "      <td>0.032899</td>\n",
       "      <td>0.909467</td>\n",
       "      <td>0.520503</td>\n",
       "      <td>1.553159</td>\n",
       "      <td>0.971628</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.903166</td>\n",
       "      <td>0.471628</td>\n",
       "      <td>0.472823</td>\n",
       "      <td>0.014218</td>\n",
       "      <td>8.778483e-04</td>\n",
       "      <td>1.182425e-02</td>\n",
       "      <td>0.005305</td>\n",
       "      <td>0.312446</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>5.267090e-03</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.003820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdvancedNegGrad</td>\n",
       "      <td>0.731277</td>\n",
       "      <td>0.484171</td>\n",
       "      <td>0.748774</td>\n",
       "      <td>0.649319</td>\n",
       "      <td>38.736015</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.451893</td>\n",
       "      <td>0.135659</td>\n",
       "      <td>0.729980</td>\n",
       "      <td>0.025033</td>\n",
       "      <td>5.793799e-02</td>\n",
       "      <td>2.733872e-02</td>\n",
       "      <td>0.038509</td>\n",
       "      <td>7.792461</td>\n",
       "      <td>0.019185</td>\n",
       "      <td>5.179305e-02</td>\n",
       "      <td>0.019185</td>\n",
       "      <td>0.031701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BadTeaching</td>\n",
       "      <td>0.493519</td>\n",
       "      <td>0.487896</td>\n",
       "      <td>0.494120</td>\n",
       "      <td>0.561275</td>\n",
       "      <td>1.553159</td>\n",
       "      <td>0.512868</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.448169</td>\n",
       "      <td>0.012868</td>\n",
       "      <td>0.733891</td>\n",
       "      <td>0.018331</td>\n",
       "      <td>3.423608e-02</td>\n",
       "      <td>1.663027e-02</td>\n",
       "      <td>0.015547</td>\n",
       "      <td>0.312446</td>\n",
       "      <td>0.007455</td>\n",
       "      <td>4.038102e-02</td>\n",
       "      <td>0.007455</td>\n",
       "      <td>0.016621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BadTeaching</td>\n",
       "      <td>0.480557</td>\n",
       "      <td>0.463687</td>\n",
       "      <td>0.482361</td>\n",
       "      <td>0.548528</td>\n",
       "      <td>1.553159</td>\n",
       "      <td>0.504264</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.472377</td>\n",
       "      <td>0.010310</td>\n",
       "      <td>0.729968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.551115e-17</td>\n",
       "      <td>5.551115e-17</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>0.312446</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.000877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BadTeaching</td>\n",
       "      <td>0.569609</td>\n",
       "      <td>0.601490</td>\n",
       "      <td>0.570408</td>\n",
       "      <td>0.626623</td>\n",
       "      <td>6.640678</td>\n",
       "      <td>0.517287</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.334575</td>\n",
       "      <td>0.017287</td>\n",
       "      <td>0.767518</td>\n",
       "      <td>0.030971</td>\n",
       "      <td>3.950317e-02</td>\n",
       "      <td>3.298564e-02</td>\n",
       "      <td>0.027438</td>\n",
       "      <td>5.158093</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>3.335823e-02</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.016253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Finetuning</td>\n",
       "      <td>0.922665</td>\n",
       "      <td>0.953445</td>\n",
       "      <td>0.937631</td>\n",
       "      <td>0.970039</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>0.534109</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.017381</td>\n",
       "      <td>0.034109</td>\n",
       "      <td>0.927224</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>2.633545e-03</td>\n",
       "      <td>1.041413e-03</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>0.156613</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>3.511393e-03</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.004541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Finetuning</td>\n",
       "      <td>0.924101</td>\n",
       "      <td>0.954687</td>\n",
       "      <td>0.949752</td>\n",
       "      <td>0.971609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.529690</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.018622</td>\n",
       "      <td>0.029690</td>\n",
       "      <td>0.932361</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>3.511393e-03</td>\n",
       "      <td>7.956140e-04</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>2.633545e-03</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.002166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Finetuning</td>\n",
       "      <td>0.927889</td>\n",
       "      <td>0.970826</td>\n",
       "      <td>0.945004</td>\n",
       "      <td>0.963734</td>\n",
       "      <td>0.446841</td>\n",
       "      <td>0.533643</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.034761</td>\n",
       "      <td>0.033643</td>\n",
       "      <td>0.930301</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>1.755697e-03</td>\n",
       "      <td>4.269148e-04</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.312446</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>4.389241e-03</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Finetuning</td>\n",
       "      <td>0.924028</td>\n",
       "      <td>0.968343</td>\n",
       "      <td>0.937905</td>\n",
       "      <td>0.958838</td>\n",
       "      <td>2.491435</td>\n",
       "      <td>0.544806</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.032278</td>\n",
       "      <td>0.044806</td>\n",
       "      <td>0.917208</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>7.900634e-03</td>\n",
       "      <td>1.109332e-03</td>\n",
       "      <td>0.007923</td>\n",
       "      <td>3.518723</td>\n",
       "      <td>0.005372</td>\n",
       "      <td>1.755697e-03</td>\n",
       "      <td>0.005372</td>\n",
       "      <td>0.006180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Finetuning</td>\n",
       "      <td>0.923854</td>\n",
       "      <td>0.967722</td>\n",
       "      <td>0.937581</td>\n",
       "      <td>0.959103</td>\n",
       "      <td>4.647323</td>\n",
       "      <td>0.542481</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.031657</td>\n",
       "      <td>0.042481</td>\n",
       "      <td>0.919447</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>8.778483e-03</td>\n",
       "      <td>1.222529e-03</td>\n",
       "      <td>0.009163</td>\n",
       "      <td>6.567608</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>2.633545e-03</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.001028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Finetuning</td>\n",
       "      <td>0.922876</td>\n",
       "      <td>0.969584</td>\n",
       "      <td>0.936927</td>\n",
       "      <td>0.955537</td>\n",
       "      <td>5.808185</td>\n",
       "      <td>0.544264</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.044264</td>\n",
       "      <td>0.917174</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>1.481265e-03</td>\n",
       "      <td>0.007862</td>\n",
       "      <td>8.209316</td>\n",
       "      <td>0.004824</td>\n",
       "      <td>5.238750e-17</td>\n",
       "      <td>0.004824</td>\n",
       "      <td>0.006059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FisherForgetting</td>\n",
       "      <td>0.851990</td>\n",
       "      <td>0.881440</td>\n",
       "      <td>0.856818</td>\n",
       "      <td>0.902777</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>0.536357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.054624</td>\n",
       "      <td>0.036357</td>\n",
       "      <td>0.889638</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>2.984684e-02</td>\n",
       "      <td>3.645594e-02</td>\n",
       "      <td>0.034580</td>\n",
       "      <td>0.156613</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>3.599178e-02</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.014001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FisherForgetting</td>\n",
       "      <td>0.819717</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.824126</td>\n",
       "      <td>0.866416</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>0.529845</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.029845</td>\n",
       "      <td>0.880014</td>\n",
       "      <td>0.062705</td>\n",
       "      <td>6.583862e-02</td>\n",
       "      <td>6.560258e-02</td>\n",
       "      <td>0.055403</td>\n",
       "      <td>0.156613</td>\n",
       "      <td>0.008990</td>\n",
       "      <td>5.267090e-02</td>\n",
       "      <td>0.008990</td>\n",
       "      <td>0.022363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FisherForgetting</td>\n",
       "      <td>0.768472</td>\n",
       "      <td>0.829299</td>\n",
       "      <td>0.772053</td>\n",
       "      <td>0.798519</td>\n",
       "      <td>0.612679</td>\n",
       "      <td>0.546589</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.106766</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.837647</td>\n",
       "      <td>0.080234</td>\n",
       "      <td>6.408292e-02</td>\n",
       "      <td>8.395991e-02</td>\n",
       "      <td>0.086343</td>\n",
       "      <td>0.077917</td>\n",
       "      <td>0.010086</td>\n",
       "      <td>7.022786e-02</td>\n",
       "      <td>0.010086</td>\n",
       "      <td>0.050203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>eu_k</td>\n",
       "      <td>0.923983</td>\n",
       "      <td>0.970826</td>\n",
       "      <td>0.938113</td>\n",
       "      <td>0.956468</td>\n",
       "      <td>0.501386</td>\n",
       "      <td>0.524264</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.034761</td>\n",
       "      <td>0.024264</td>\n",
       "      <td>0.937728</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>7.022786e-03</td>\n",
       "      <td>1.064053e-03</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>0.704368</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>8.778483e-04</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.005156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Identity</td>\n",
       "      <td>0.922775</td>\n",
       "      <td>0.970205</td>\n",
       "      <td>0.936366</td>\n",
       "      <td>0.954771</td>\n",
       "      <td>0.003873</td>\n",
       "      <td>0.544419</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>0.044419</td>\n",
       "      <td>0.916969</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>5.267090e-03</td>\n",
       "      <td>1.824091e-03</td>\n",
       "      <td>0.007138</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>8.778483e-04</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>0.007839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NegGrad</td>\n",
       "      <td>0.922812</td>\n",
       "      <td>0.970205</td>\n",
       "      <td>0.936346</td>\n",
       "      <td>0.954837</td>\n",
       "      <td>0.003873</td>\n",
       "      <td>0.542016</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>0.042016</td>\n",
       "      <td>0.919390</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>5.267090e-03</td>\n",
       "      <td>1.843496e-03</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>8.778483e-04</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.006910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NegGrad</td>\n",
       "      <td>0.923141</td>\n",
       "      <td>0.963998</td>\n",
       "      <td>0.936531</td>\n",
       "      <td>0.961174</td>\n",
       "      <td>1.718996</td>\n",
       "      <td>0.542713</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.027933</td>\n",
       "      <td>0.042713</td>\n",
       "      <td>0.918857</td>\n",
       "      <td>0.002846</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>2.027845e-03</td>\n",
       "      <td>0.008666</td>\n",
       "      <td>0.546976</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.004164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NegGrad</td>\n",
       "      <td>0.785989</td>\n",
       "      <td>0.752948</td>\n",
       "      <td>0.790216</td>\n",
       "      <td>0.835228</td>\n",
       "      <td>1.774643</td>\n",
       "      <td>0.497984</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.183116</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.890979</td>\n",
       "      <td>0.042083</td>\n",
       "      <td>3.072469e-02</td>\n",
       "      <td>4.263650e-02</td>\n",
       "      <td>0.034262</td>\n",
       "      <td>0.155834</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>2.457975e-02</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.020603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Scrub</td>\n",
       "      <td>0.921220</td>\n",
       "      <td>0.955928</td>\n",
       "      <td>0.941006</td>\n",
       "      <td>0.964981</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.539380</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.019863</td>\n",
       "      <td>0.039380</td>\n",
       "      <td>0.921230</td>\n",
       "      <td>0.003351</td>\n",
       "      <td>1.755697e-03</td>\n",
       "      <td>4.447029e-03</td>\n",
       "      <td>0.005582</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>4.389241e-03</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.004526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Scrub</td>\n",
       "      <td>0.919006</td>\n",
       "      <td>0.944134</td>\n",
       "      <td>0.929357</td>\n",
       "      <td>0.971873</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>0.529380</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.008070</td>\n",
       "      <td>0.029380</td>\n",
       "      <td>0.930123</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.293759e-03</td>\n",
       "      <td>0.012296</td>\n",
       "      <td>0.156613</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.003664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Scrub</td>\n",
       "      <td>0.519443</td>\n",
       "      <td>0.536313</td>\n",
       "      <td>0.517639</td>\n",
       "      <td>0.586770</td>\n",
       "      <td>12.567302</td>\n",
       "      <td>0.502171</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.399752</td>\n",
       "      <td>0.004031</td>\n",
       "      <td>0.755691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>1.495224</td>\n",
       "      <td>0.005043</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>0.003727</td>\n",
       "      <td>0.003727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SelectiveSynapticDampening</td>\n",
       "      <td>0.918942</td>\n",
       "      <td>0.966480</td>\n",
       "      <td>0.931356</td>\n",
       "      <td>0.950988</td>\n",
       "      <td>0.889809</td>\n",
       "      <td>0.542713</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.030416</td>\n",
       "      <td>0.042713</td>\n",
       "      <td>0.916758</td>\n",
       "      <td>0.004631</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.935394e-03</td>\n",
       "      <td>0.006237</td>\n",
       "      <td>0.625672</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.000123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SelectiveSynapticDampening</td>\n",
       "      <td>0.918942</td>\n",
       "      <td>0.966480</td>\n",
       "      <td>0.931356</td>\n",
       "      <td>0.950988</td>\n",
       "      <td>0.889809</td>\n",
       "      <td>0.548217</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.030416</td>\n",
       "      <td>0.048217</td>\n",
       "      <td>0.911254</td>\n",
       "      <td>0.004631</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.935394e-03</td>\n",
       "      <td>0.006237</td>\n",
       "      <td>0.625672</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.002864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SelectiveSynapticDampening</td>\n",
       "      <td>0.918933</td>\n",
       "      <td>0.966480</td>\n",
       "      <td>0.931356</td>\n",
       "      <td>0.950970</td>\n",
       "      <td>0.889809</td>\n",
       "      <td>0.544961</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.030416</td>\n",
       "      <td>0.044961</td>\n",
       "      <td>0.914505</td>\n",
       "      <td>0.004618</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.935394e-03</td>\n",
       "      <td>0.006213</td>\n",
       "      <td>0.625672</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.005160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SelectiveSynapticDampening</td>\n",
       "      <td>0.918933</td>\n",
       "      <td>0.966480</td>\n",
       "      <td>0.931356</td>\n",
       "      <td>0.950970</td>\n",
       "      <td>0.889809</td>\n",
       "      <td>0.546977</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.030416</td>\n",
       "      <td>0.046977</td>\n",
       "      <td>0.912490</td>\n",
       "      <td>0.004618</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.935394e-03</td>\n",
       "      <td>0.006213</td>\n",
       "      <td>0.625672</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>0.004283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Cascade</td>\n",
       "      <td>0.924248</td>\n",
       "      <td>0.934823</td>\n",
       "      <td>0.927159</td>\n",
       "      <td>0.991061</td>\n",
       "      <td>1.165837</td>\n",
       "      <td>0.510775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013656</td>\n",
       "      <td>0.010775</td>\n",
       "      <td>0.951349</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>7.900634e-03</td>\n",
       "      <td>8.538296e-04</td>\n",
       "      <td>0.009304</td>\n",
       "      <td>0.234530</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>3.511393e-03</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.000870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Cascade</td>\n",
       "      <td>0.924842</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.927136</td>\n",
       "      <td>0.988528</td>\n",
       "      <td>1.165837</td>\n",
       "      <td>0.515736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>0.015736</td>\n",
       "      <td>0.946685</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>5.267090e-03</td>\n",
       "      <td>6.630117e-04</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>0.234530</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>8.778483e-04</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>0.002071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Cascade</td>\n",
       "      <td>0.924495</td>\n",
       "      <td>0.939168</td>\n",
       "      <td>0.927223</td>\n",
       "      <td>0.987231</td>\n",
       "      <td>1.165837</td>\n",
       "      <td>0.520233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003104</td>\n",
       "      <td>0.020233</td>\n",
       "      <td>0.942015</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>1.755697e-03</td>\n",
       "      <td>1.778812e-04</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>0.234530</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>4.389241e-03</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>SuccessiveRandomLabels</td>\n",
       "      <td>0.921504</td>\n",
       "      <td>0.939789</td>\n",
       "      <td>0.924527</td>\n",
       "      <td>0.980793</td>\n",
       "      <td>1.387321</td>\n",
       "      <td>0.503953</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>0.003953</td>\n",
       "      <td>0.956798</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>8.778483e-04</td>\n",
       "      <td>3.234203e-04</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.077917</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>5.267090e-03</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.000741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>SuccessiveRandomLabels</td>\n",
       "      <td>0.922345</td>\n",
       "      <td>0.946617</td>\n",
       "      <td>0.936449</td>\n",
       "      <td>0.975884</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>0.526202</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.010552</td>\n",
       "      <td>0.026202</td>\n",
       "      <td>0.934971</td>\n",
       "      <td>0.001992</td>\n",
       "      <td>8.778483e-04</td>\n",
       "      <td>2.464463e-03</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.156613</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>7.022786e-03</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.000210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>SuccessiveRandomLabels</td>\n",
       "      <td>0.923946</td>\n",
       "      <td>0.950962</td>\n",
       "      <td>0.949496</td>\n",
       "      <td>0.974833</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.520310</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.014898</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.941663</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>8.778483e-04</td>\n",
       "      <td>7.018221e-04</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>5.267090e-03</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>0.003405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Cascade</td>\n",
       "      <td>0.923397</td>\n",
       "      <td>0.951583</td>\n",
       "      <td>0.938267</td>\n",
       "      <td>0.973192</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>0.523798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015518</td>\n",
       "      <td>0.023798</td>\n",
       "      <td>0.937900</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>2.633545e-03</td>\n",
       "      <td>6.306696e-04</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.156613</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>3.511393e-03</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>0.003192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Cascade</td>\n",
       "      <td>0.922583</td>\n",
       "      <td>0.933582</td>\n",
       "      <td>0.925416</td>\n",
       "      <td>0.988946</td>\n",
       "      <td>2.880961</td>\n",
       "      <td>0.512403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007449</td>\n",
       "      <td>0.012403</td>\n",
       "      <td>0.948888</td>\n",
       "      <td>0.001514</td>\n",
       "      <td>8.778483e-04</td>\n",
       "      <td>2.357734e-03</td>\n",
       "      <td>0.006321</td>\n",
       "      <td>0.780726</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.003498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Cascade</td>\n",
       "      <td>0.924367</td>\n",
       "      <td>0.949721</td>\n",
       "      <td>0.949498</td>\n",
       "      <td>0.976825</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.526512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013656</td>\n",
       "      <td>0.026512</td>\n",
       "      <td>0.935672</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.276957e-04</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.001084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     unlearner  sklearn.metrics.accuracy_score.test.unlearned  \\\n",
       "0                    GoldModel                                       0.924166   \n",
       "1              AdvancedNegGrad                                       0.888901   \n",
       "2              AdvancedNegGrad                                       0.731277   \n",
       "3                  BadTeaching                                       0.493519   \n",
       "4                  BadTeaching                                       0.480557   \n",
       "5                  BadTeaching                                       0.569609   \n",
       "6                   Finetuning                                       0.922665   \n",
       "7                   Finetuning                                       0.924101   \n",
       "8                   Finetuning                                       0.927889   \n",
       "9                   Finetuning                                       0.924028   \n",
       "10                  Finetuning                                       0.923854   \n",
       "11                  Finetuning                                       0.922876   \n",
       "12            FisherForgetting                                       0.851990   \n",
       "13            FisherForgetting                                       0.819717   \n",
       "14            FisherForgetting                                       0.768472   \n",
       "15                        eu_k                                       0.923983   \n",
       "16                    Identity                                       0.922775   \n",
       "17                     NegGrad                                       0.922812   \n",
       "18                     NegGrad                                       0.923141   \n",
       "19                     NegGrad                                       0.785989   \n",
       "20                       Scrub                                       0.921220   \n",
       "21                       Scrub                                       0.919006   \n",
       "22                       Scrub                                       0.519443   \n",
       "23  SelectiveSynapticDampening                                       0.918942   \n",
       "24  SelectiveSynapticDampening                                       0.918942   \n",
       "25  SelectiveSynapticDampening                                       0.918933   \n",
       "26  SelectiveSynapticDampening                                       0.918933   \n",
       "27                     Cascade                                       0.924248   \n",
       "28                     Cascade                                       0.924842   \n",
       "29                     Cascade                                       0.924495   \n",
       "30      SuccessiveRandomLabels                                       0.921504   \n",
       "31      SuccessiveRandomLabels                                       0.922345   \n",
       "32      SuccessiveRandomLabels                                       0.923946   \n",
       "33                     Cascade                                       0.923397   \n",
       "34                     Cascade                                       0.922583   \n",
       "35                     Cascade                                       0.924367   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.forget.unlearned  \\\n",
       "0                                          0.936065   \n",
       "1                                          0.032899   \n",
       "2                                          0.484171   \n",
       "3                                          0.487896   \n",
       "4                                          0.463687   \n",
       "5                                          0.601490   \n",
       "6                                          0.953445   \n",
       "7                                          0.954687   \n",
       "8                                          0.970826   \n",
       "9                                          0.968343   \n",
       "10                                         0.967722   \n",
       "11                                         0.969584   \n",
       "12                                         0.881440   \n",
       "13                                         0.854749   \n",
       "14                                         0.829299   \n",
       "15                                         0.970826   \n",
       "16                                         0.970205   \n",
       "17                                         0.970205   \n",
       "18                                         0.963998   \n",
       "19                                         0.752948   \n",
       "20                                         0.955928   \n",
       "21                                         0.944134   \n",
       "22                                         0.536313   \n",
       "23                                         0.966480   \n",
       "24                                         0.966480   \n",
       "25                                         0.966480   \n",
       "26                                         0.966480   \n",
       "27                                         0.934823   \n",
       "28                                         0.938547   \n",
       "29                                         0.939168   \n",
       "30                                         0.939789   \n",
       "31                                         0.946617   \n",
       "32                                         0.950962   \n",
       "33                                         0.951583   \n",
       "34                                         0.933582   \n",
       "35                                         0.949721   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.retain.unlearned       AUS        AIN  \\\n",
       "0                                          0.937864  0.989631   0.778516   \n",
       "1                                          0.909467  0.520503   1.553159   \n",
       "2                                          0.748774  0.649319  38.736015   \n",
       "3                                          0.494120  0.561275   1.553159   \n",
       "4                                          0.482361  0.548528   1.553159   \n",
       "5                                          0.570408  0.626623   6.640678   \n",
       "6                                          0.937631  0.970039   0.778516   \n",
       "7                                          0.949752  0.971609   1.000000   \n",
       "8                                          0.945004  0.963734   0.446841   \n",
       "9                                          0.937905  0.958838   2.491435   \n",
       "10                                         0.937581  0.959103   4.647323   \n",
       "11                                         0.936927  0.955537   5.808185   \n",
       "12                                         0.856818  0.902777   0.778516   \n",
       "13                                         0.824126  0.866416   0.778516   \n",
       "14                                         0.772053  0.798519   0.612679   \n",
       "15                                         0.938113  0.956468   0.501386   \n",
       "16                                         0.936366  0.954771   0.003873   \n",
       "17                                         0.936346  0.954837   0.003873   \n",
       "18                                         0.936531  0.961174   1.718996   \n",
       "19                                         0.790216  0.835228   1.774643   \n",
       "20                                         0.941006  0.964981   1.000000   \n",
       "21                                         0.929357  0.971873   0.778516   \n",
       "22                                         0.517639  0.586770  12.567302   \n",
       "23                                         0.931356  0.950988   0.889809   \n",
       "24                                         0.931356  0.950988   0.889809   \n",
       "25                                         0.931356  0.950970   0.889809   \n",
       "26                                         0.931356  0.950970   0.889809   \n",
       "27                                         0.927159  0.991061   1.165837   \n",
       "28                                         0.927136  0.988528   1.165837   \n",
       "29                                         0.927223  0.987231   1.165837   \n",
       "30                                         0.924527  0.980793   1.387321   \n",
       "31                                         0.936449  0.975884   0.778516   \n",
       "32                                         0.949496  0.974833   1.000000   \n",
       "33                                         0.938267  0.973192   0.778516   \n",
       "34                                         0.925416  0.988946   2.880961   \n",
       "35                                         0.949498  0.976825   1.000000   \n",
       "\n",
       "        UMIA       lr  forget_diff  Unlearning_Score     nomus  \\\n",
       "0   0.508837      NaN     0.000000          0.008837  0.953246   \n",
       "1   0.971628  0.00010     0.903166          0.471628  0.472823   \n",
       "2   0.635659  0.00001     0.451893          0.135659  0.729980   \n",
       "3   0.512868  0.00100     0.448169          0.012868  0.733891   \n",
       "4   0.504264  0.00010     0.472377          0.010310  0.729968   \n",
       "5   0.517287  0.00001     0.334575          0.017287  0.767518   \n",
       "6   0.534109  0.00100     0.017381          0.034109  0.927224   \n",
       "7   0.529690  0.00010     0.018622          0.029690  0.932361   \n",
       "8   0.533643  0.00001     0.034761          0.033643  0.930301   \n",
       "9   0.544806  0.00100     0.032278          0.044806  0.917208   \n",
       "10  0.542481  0.00010     0.031657          0.042481  0.919447   \n",
       "11  0.544264  0.00001     0.033520          0.044264  0.917174   \n",
       "12  0.536357      NaN     0.054624          0.036357  0.889638   \n",
       "13  0.529845      NaN     0.094972          0.029845  0.880014   \n",
       "14  0.546589      NaN     0.106766          0.046589  0.837647   \n",
       "15  0.524264  0.01000     0.034761          0.024264  0.937728   \n",
       "16  0.544419      NaN     0.034140          0.044419  0.916969   \n",
       "17  0.542016  0.00001     0.034140          0.042016  0.919390   \n",
       "18  0.542713  0.00010     0.027933          0.042713  0.918857   \n",
       "19  0.497984  0.00100     0.183116          0.002016  0.890979   \n",
       "20  0.539380  0.00010     0.019863          0.039380  0.921230   \n",
       "21  0.529380  0.00100     0.008070          0.029380  0.930123   \n",
       "22  0.502171  0.01000     0.399752          0.004031  0.755691   \n",
       "23  0.542713  0.01000     0.030416          0.042713  0.916758   \n",
       "24  0.548217  0.01000     0.030416          0.048217  0.911254   \n",
       "25  0.544961  0.01000     0.030416          0.044961  0.914505   \n",
       "26  0.546977  0.01000     0.030416          0.046977  0.912490   \n",
       "27  0.510775      NaN     0.013656          0.010775  0.951349   \n",
       "28  0.515736      NaN     0.002483          0.015736  0.946685   \n",
       "29  0.520233      NaN     0.003104          0.020233  0.942015   \n",
       "30  0.503953  0.01000     0.003724          0.003953  0.956798   \n",
       "31  0.526202  0.00100     0.010552          0.026202  0.934971   \n",
       "32  0.520310  0.00010     0.014898          0.020310  0.941663   \n",
       "33  0.523798      NaN     0.015518          0.023798  0.937900   \n",
       "34  0.512403      NaN     0.007449          0.012403  0.948888   \n",
       "35  0.526512      NaN     0.013656          0.026512  0.935672   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.test.unlearned_std  \\\n",
       "0                                            0.000362   \n",
       "1                                            0.014218   \n",
       "2                                            0.025033   \n",
       "3                                            0.018331   \n",
       "4                                            0.000000   \n",
       "5                                            0.030971   \n",
       "6                                            0.000970   \n",
       "7                                            0.000492   \n",
       "8                                            0.000556   \n",
       "9                                            0.001617   \n",
       "10                                           0.001837   \n",
       "11                                           0.002471   \n",
       "12                                           0.034140   \n",
       "13                                           0.062705   \n",
       "14                                           0.080234   \n",
       "15                                           0.001746   \n",
       "16                                           0.002523   \n",
       "17                                           0.002419   \n",
       "18                                           0.002846   \n",
       "19                                           0.042083   \n",
       "20                                           0.003351   \n",
       "21                                           0.005136   \n",
       "22                                           0.000000   \n",
       "23                                           0.004631   \n",
       "24                                           0.004631   \n",
       "25                                           0.004618   \n",
       "26                                           0.004618   \n",
       "27                                           0.000453   \n",
       "28                                           0.000414   \n",
       "29                                           0.000750   \n",
       "30                                           0.000492   \n",
       "31                                           0.001992   \n",
       "32                                           0.000207   \n",
       "33                                           0.000026   \n",
       "34                                           0.001514   \n",
       "35                                           0.000194   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.forget.unlearned_std  \\\n",
       "0                                        6.144938e-03     \n",
       "1                                        8.778483e-04     \n",
       "2                                        5.793799e-02     \n",
       "3                                        3.423608e-02     \n",
       "4                                        5.551115e-17     \n",
       "5                                        3.950317e-02     \n",
       "6                                        2.633545e-03     \n",
       "7                                        3.511393e-03     \n",
       "8                                        1.755697e-03     \n",
       "9                                        7.900634e-03     \n",
       "10                                       8.778483e-03     \n",
       "11                                       6.144938e-03     \n",
       "12                                       2.984684e-02     \n",
       "13                                       6.583862e-02     \n",
       "14                                       6.408292e-02     \n",
       "15                                       7.022786e-03     \n",
       "16                                       5.267090e-03     \n",
       "17                                       5.267090e-03     \n",
       "18                                       6.144938e-03     \n",
       "19                                       3.072469e-02     \n",
       "20                                       1.755697e-03     \n",
       "21                                       0.000000e+00     \n",
       "22                                       0.000000e+00     \n",
       "23                                       0.000000e+00     \n",
       "24                                       0.000000e+00     \n",
       "25                                       0.000000e+00     \n",
       "26                                       0.000000e+00     \n",
       "27                                       7.900634e-03     \n",
       "28                                       5.267090e-03     \n",
       "29                                       1.755697e-03     \n",
       "30                                       8.778483e-04     \n",
       "31                                       8.778483e-04     \n",
       "32                                       8.778483e-04     \n",
       "33                                       2.633545e-03     \n",
       "34                                       8.778483e-04     \n",
       "35                                       0.000000e+00     \n",
       "\n",
       "    sklearn.metrics.accuracy_score.retain.unlearned_std   AUS_std   AIN_std  \\\n",
       "0                                        1.487733e-04    0.002816  0.156613   \n",
       "1                                        1.182425e-02    0.005305  0.312446   \n",
       "2                                        2.733872e-02    0.038509  7.792461   \n",
       "3                                        1.663027e-02    0.015547  0.312446   \n",
       "4                                        5.551115e-17    0.002481  0.312446   \n",
       "5                                        3.298564e-02    0.027438  5.158093   \n",
       "6                                        1.041413e-03    0.001890  0.156613   \n",
       "7                                        7.956140e-04    0.000078  0.000000   \n",
       "8                                        4.269148e-04    0.000251  0.312446   \n",
       "9                                        1.109332e-03    0.007923  3.518723   \n",
       "10                                       1.222529e-03    0.009163  6.567608   \n",
       "11                                       1.481265e-03    0.007862  8.209316   \n",
       "12                                       3.645594e-02    0.034580  0.156613   \n",
       "13                                       6.560258e-02    0.055403  0.156613   \n",
       "14                                       8.395991e-02    0.086343  0.077917   \n",
       "15                                       1.064053e-03    0.007314  0.704368   \n",
       "16                                       1.824091e-03    0.007138  0.000779   \n",
       "17                                       1.843496e-03    0.006944  0.000779   \n",
       "18                                       2.027845e-03    0.008666  0.546976   \n",
       "19                                       4.263650e-02    0.034262  0.155834   \n",
       "20                                       4.447029e-03    0.005582  0.000000   \n",
       "21                                       6.293759e-03    0.012296  0.156613   \n",
       "22                                       0.000000e+00    0.002481  1.495224   \n",
       "23                                       4.935394e-03    0.006237  0.625672   \n",
       "24                                       4.935394e-03    0.006237  0.625672   \n",
       "25                                       4.935394e-03    0.006213  0.625672   \n",
       "26                                       4.935394e-03    0.006213  0.625672   \n",
       "27                                       8.538296e-04    0.009304  0.234530   \n",
       "28                                       6.630117e-04    0.001842  0.234530   \n",
       "29                                       1.778812e-04    0.002249  0.234530   \n",
       "30                                       3.234203e-04    0.000676  0.077917   \n",
       "31                                       2.464463e-03    0.000544  0.156613   \n",
       "32                                       7.018221e-04    0.002022  0.000000   \n",
       "33                                       6.306696e-04    0.000089  0.156613   \n",
       "34                                       2.357734e-03    0.006321  0.780726   \n",
       "35                                       7.276957e-04    0.002086  0.000000   \n",
       "\n",
       "    UMIA_std  forget_diff_std  Unlearning_Score_std  nomus_std  \n",
       "0   0.000000     0.000000e+00              0.000000   0.000181  \n",
       "1   0.003289     5.267090e-03              0.003289   0.003820  \n",
       "2   0.019185     5.179305e-02              0.019185   0.031701  \n",
       "3   0.007455     4.038102e-02              0.007455   0.016621  \n",
       "4   0.009428     6.144938e-03              0.000877   0.000877  \n",
       "5   0.000767     3.335823e-02              0.000767   0.016253  \n",
       "6   0.004056     3.511393e-03              0.004056   0.004541  \n",
       "7   0.002412     2.633545e-03              0.002412   0.002166  \n",
       "8   0.000548     4.389241e-03              0.000548   0.000270  \n",
       "9   0.005372     1.755697e-03              0.005372   0.006180  \n",
       "10  0.000110     2.633545e-03              0.000110   0.001028  \n",
       "11  0.004824     5.238750e-17              0.004824   0.006059  \n",
       "12  0.003070     3.599178e-02              0.003070   0.014001  \n",
       "13  0.008990     5.267090e-02              0.008990   0.022363  \n",
       "14  0.010086     7.022786e-02              0.010086   0.050203  \n",
       "15  0.006030     8.778483e-04              0.006030   0.005156  \n",
       "16  0.006578     8.778483e-04              0.006578   0.007839  \n",
       "17  0.005701     8.778483e-04              0.005701   0.006910  \n",
       "18  0.002741     0.000000e+00              0.002741   0.004164  \n",
       "19  0.000439     2.457975e-02              0.000439   0.020603  \n",
       "20  0.002850     4.389241e-03              0.002850   0.004526  \n",
       "21  0.001096     6.144938e-03              0.001096   0.003664  \n",
       "22  0.005043     6.144938e-03              0.003727   0.003727  \n",
       "23  0.002193     6.144938e-03              0.002193   0.000123  \n",
       "24  0.000548     6.144938e-03              0.000548   0.002864  \n",
       "25  0.002850     6.144938e-03              0.002850   0.005160  \n",
       "26  0.001973     6.144938e-03              0.001973   0.004283  \n",
       "27  0.001096     3.511393e-03              0.001096   0.000870  \n",
       "28  0.001864     8.778483e-04              0.001864   0.002071  \n",
       "29  0.000329     4.389241e-03              0.000329   0.000046  \n",
       "30  0.000987     5.267090e-03              0.000987   0.000741  \n",
       "31  0.001206     7.022786e-03              0.001206   0.000210  \n",
       "32  0.003508     5.267090e-03              0.003508   0.003405  \n",
       "33  0.003179     3.511393e-03              0.003179   0.003192  \n",
       "34  0.002741     0.000000e+00              0.002741   0.003498  \n",
       "35  0.000987     6.144938e-03              0.000987   0.001084  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_mean_std_for = ['sklearn.metrics.accuracy_score.test.unlearned', 'sklearn.metrics.accuracy_score.forget.unlearned', 'sklearn.metrics.accuracy_score.retain.unlearned', 'AUS', 'AIN', 'UMIA', 'forget_diff', 'Unlearning_Score', 'nomus']\n",
    "\n",
    "new_df = pd.DataFrame(columns=names)\n",
    "\n",
    "for name in names:\n",
    "    if name in evaluate_mean_std_for:\n",
    "        new_df[name] = np.mean([df[name] for df in datasets], axis=0)\n",
    "        new_df[name + '_std'] = np.std([df[name] for df in datasets], axis=0)\n",
    "    else:\n",
    "        new_df[name] = datasets[0][name]\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order columns name of the df \n",
    "\n",
    "new_df = new_df[['unlearner', 'lr', 'sklearn.metrics.accuracy_score.test.unlearned', 'sklearn.metrics.accuracy_score.test.unlearned_std', 'sklearn.metrics.accuracy_score.forget.unlearned', 'sklearn.metrics.accuracy_score.forget.unlearned_std', 'sklearn.metrics.accuracy_score.retain.unlearned', 'sklearn.metrics.accuracy_score.retain.unlearned_std', 'AUS', 'AUS_std', 'AIN', 'AIN_std', 'forget_diff', 'forget_diff_std', 'UMIA', 'UMIA_std', 'Unlearning_Score', 'Unlearning_Score_std', 'nomus', 'nomus_std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_316471/347910662.py:6: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  new_df['unlearner'][9] = \"cfk\"\n",
      "/tmp/ipykernel_316471/347910662.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['unlearner'][9] = \"cfk\"\n",
      "/tmp/ipykernel_316471/347910662.py:7: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  new_df['unlearner'][10] = \"cfk\"\n",
      "/tmp/ipykernel_316471/347910662.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['unlearner'][10] = \"cfk\"\n",
      "/tmp/ipykernel_316471/347910662.py:8: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  new_df['unlearner'][11] = \"cfk\"\n",
      "/tmp/ipykernel_316471/347910662.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['unlearner'][11] = \"cfk\"\n",
      "/tmp/ipykernel_316471/347910662.py:11: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  new_df['lr'][12] = '1.00E-07'\n",
      "/tmp/ipykernel_316471/347910662.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['lr'][12] = '1.00E-07'\n",
      "/tmp/ipykernel_316471/347910662.py:11: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.00E-07' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  new_df['lr'][12] = '1.00E-07'\n",
      "/tmp/ipykernel_316471/347910662.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['lr'][13] = '1.00E-08'\n",
      "/tmp/ipykernel_316471/347910662.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['lr'][14] = '1.00E-06'\n",
      "/tmp/ipykernel_316471/347910662.py:16: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  new_df['unlearner'][27] = 'unsir'\n",
      "/tmp/ipykernel_316471/347910662.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['unlearner'][27] = 'unsir'\n",
      "/tmp/ipykernel_316471/347910662.py:17: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  new_df['unlearner'][28] = 'unsir'\n",
      "/tmp/ipykernel_316471/347910662.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['unlearner'][28] = 'unsir'\n",
      "/tmp/ipykernel_316471/347910662.py:18: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  new_df['unlearner'][29] = 'unsir'\n",
      "/tmp/ipykernel_316471/347910662.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['unlearner'][29] = 'unsir'\n",
      "/tmp/ipykernel_316471/347910662.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['lr'][27] = 0.001\n",
      "/tmp/ipykernel_316471/347910662.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['lr'][28] = 0.01\n",
      "/tmp/ipykernel_316471/347910662.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['lr'][29] = 0.0001\n",
      "/tmp/ipykernel_316471/347910662.py:24: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  new_df['unlearner'][33] = 'salun'\n",
      "/tmp/ipykernel_316471/347910662.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['unlearner'][33] = 'salun'\n",
      "/tmp/ipykernel_316471/347910662.py:25: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  new_df['unlearner'][34] = 'salun'\n",
      "/tmp/ipykernel_316471/347910662.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['unlearner'][34] = 'salun'\n",
      "/tmp/ipykernel_316471/347910662.py:26: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  new_df['unlearner'][35] = 'salun'\n",
      "/tmp/ipykernel_316471/347910662.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['unlearner'][35] = 'salun'\n",
      "/tmp/ipykernel_316471/347910662.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['lr'][33] = 0.001\n",
      "/tmp/ipykernel_316471/347910662.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['lr'][34] = 0.01\n",
      "/tmp/ipykernel_316471/347910662.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['lr'][35] = 0.0001\n",
      "/tmp/ipykernel_316471/347910662.py:31: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  new_df['unlearner'][23] = \"ssd\"\n",
      "/tmp/ipykernel_316471/347910662.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['unlearner'][23] = \"ssd\"\n",
      "/tmp/ipykernel_316471/347910662.py:32: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  new_df['unlearner'][24] = \"ssd\"\n",
      "/tmp/ipykernel_316471/347910662.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['unlearner'][24] = \"ssd\"\n",
      "/tmp/ipykernel_316471/347910662.py:33: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  new_df['unlearner'][25] = \"ssd\"\n",
      "/tmp/ipykernel_316471/347910662.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['unlearner'][25] = \"ssd\"\n",
      "/tmp/ipykernel_316471/347910662.py:34: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  new_df['unlearner'][26] = \"ssd\"\n",
      "/tmp/ipykernel_316471/347910662.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['unlearner'][26] = \"ssd\"\n",
      "/tmp/ipykernel_316471/347910662.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['lr'][23] = \"0.1_20\"\n",
      "/tmp/ipykernel_316471/347910662.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['lr'][24] = \"0.01_20\"\n",
      "/tmp/ipykernel_316471/347910662.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['lr'][25] = \"0.1_40\"\n",
      "/tmp/ipykernel_316471/347910662.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['lr'][26] = \"0.01_40\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unlearner</th>\n",
       "      <th>lr</th>\n",
       "      <th>sklearn.metrics.accuracy_score.test.unlearned</th>\n",
       "      <th>sklearn.metrics.accuracy_score.test.unlearned_std</th>\n",
       "      <th>sklearn.metrics.accuracy_score.forget.unlearned</th>\n",
       "      <th>sklearn.metrics.accuracy_score.forget.unlearned_std</th>\n",
       "      <th>sklearn.metrics.accuracy_score.retain.unlearned</th>\n",
       "      <th>sklearn.metrics.accuracy_score.retain.unlearned_std</th>\n",
       "      <th>AUS</th>\n",
       "      <th>AUS_std</th>\n",
       "      <th>AIN</th>\n",
       "      <th>AIN_std</th>\n",
       "      <th>forget_diff</th>\n",
       "      <th>forget_diff_std</th>\n",
       "      <th>UMIA</th>\n",
       "      <th>UMIA_std</th>\n",
       "      <th>Unlearning_Score</th>\n",
       "      <th>Unlearning_Score_std</th>\n",
       "      <th>nomus</th>\n",
       "      <th>nomus_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GoldModel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.924166</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.936065</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>0.937864</td>\n",
       "      <td>1.487733e-04</td>\n",
       "      <td>0.989631</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>0.156613</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.508837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.953246</td>\n",
       "      <td>0.000181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdvancedNegGrad</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.888901</td>\n",
       "      <td>0.014218</td>\n",
       "      <td>0.032899</td>\n",
       "      <td>8.778483e-04</td>\n",
       "      <td>0.909467</td>\n",
       "      <td>1.182425e-02</td>\n",
       "      <td>0.520503</td>\n",
       "      <td>0.005305</td>\n",
       "      <td>1.553159</td>\n",
       "      <td>0.312446</td>\n",
       "      <td>0.903166</td>\n",
       "      <td>5.267090e-03</td>\n",
       "      <td>0.971628</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.471628</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.472823</td>\n",
       "      <td>0.003820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdvancedNegGrad</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.731277</td>\n",
       "      <td>0.025033</td>\n",
       "      <td>0.484171</td>\n",
       "      <td>5.793799e-02</td>\n",
       "      <td>0.748774</td>\n",
       "      <td>2.733872e-02</td>\n",
       "      <td>0.649319</td>\n",
       "      <td>0.038509</td>\n",
       "      <td>38.736015</td>\n",
       "      <td>7.792461</td>\n",
       "      <td>0.451893</td>\n",
       "      <td>5.179305e-02</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>0.019185</td>\n",
       "      <td>0.135659</td>\n",
       "      <td>0.019185</td>\n",
       "      <td>0.729980</td>\n",
       "      <td>0.031701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BadTeaching</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.493519</td>\n",
       "      <td>0.018331</td>\n",
       "      <td>0.487896</td>\n",
       "      <td>3.423608e-02</td>\n",
       "      <td>0.494120</td>\n",
       "      <td>1.663027e-02</td>\n",
       "      <td>0.561275</td>\n",
       "      <td>0.015547</td>\n",
       "      <td>1.553159</td>\n",
       "      <td>0.312446</td>\n",
       "      <td>0.448169</td>\n",
       "      <td>4.038102e-02</td>\n",
       "      <td>0.512868</td>\n",
       "      <td>0.007455</td>\n",
       "      <td>0.012868</td>\n",
       "      <td>0.007455</td>\n",
       "      <td>0.733891</td>\n",
       "      <td>0.016621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BadTeaching</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.480557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.463687</td>\n",
       "      <td>5.551115e-17</td>\n",
       "      <td>0.482361</td>\n",
       "      <td>5.551115e-17</td>\n",
       "      <td>0.548528</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>1.553159</td>\n",
       "      <td>0.312446</td>\n",
       "      <td>0.472377</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>0.504264</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>0.010310</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.729968</td>\n",
       "      <td>0.000877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BadTeaching</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.569609</td>\n",
       "      <td>0.030971</td>\n",
       "      <td>0.601490</td>\n",
       "      <td>3.950317e-02</td>\n",
       "      <td>0.570408</td>\n",
       "      <td>3.298564e-02</td>\n",
       "      <td>0.626623</td>\n",
       "      <td>0.027438</td>\n",
       "      <td>6.640678</td>\n",
       "      <td>5.158093</td>\n",
       "      <td>0.334575</td>\n",
       "      <td>3.335823e-02</td>\n",
       "      <td>0.517287</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.017287</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.767518</td>\n",
       "      <td>0.016253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Finetuning</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.922665</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.953445</td>\n",
       "      <td>2.633545e-03</td>\n",
       "      <td>0.937631</td>\n",
       "      <td>1.041413e-03</td>\n",
       "      <td>0.970039</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>0.156613</td>\n",
       "      <td>0.017381</td>\n",
       "      <td>3.511393e-03</td>\n",
       "      <td>0.534109</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.034109</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.927224</td>\n",
       "      <td>0.004541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Finetuning</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.924101</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.954687</td>\n",
       "      <td>3.511393e-03</td>\n",
       "      <td>0.949752</td>\n",
       "      <td>7.956140e-04</td>\n",
       "      <td>0.971609</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018622</td>\n",
       "      <td>2.633545e-03</td>\n",
       "      <td>0.529690</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.029690</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.932361</td>\n",
       "      <td>0.002166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Finetuning</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.927889</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.970826</td>\n",
       "      <td>1.755697e-03</td>\n",
       "      <td>0.945004</td>\n",
       "      <td>4.269148e-04</td>\n",
       "      <td>0.963734</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.446841</td>\n",
       "      <td>0.312446</td>\n",
       "      <td>0.034761</td>\n",
       "      <td>4.389241e-03</td>\n",
       "      <td>0.533643</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.033643</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.930301</td>\n",
       "      <td>0.000270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cfk</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.924028</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.968343</td>\n",
       "      <td>7.900634e-03</td>\n",
       "      <td>0.937905</td>\n",
       "      <td>1.109332e-03</td>\n",
       "      <td>0.958838</td>\n",
       "      <td>0.007923</td>\n",
       "      <td>2.491435</td>\n",
       "      <td>3.518723</td>\n",
       "      <td>0.032278</td>\n",
       "      <td>1.755697e-03</td>\n",
       "      <td>0.544806</td>\n",
       "      <td>0.005372</td>\n",
       "      <td>0.044806</td>\n",
       "      <td>0.005372</td>\n",
       "      <td>0.917208</td>\n",
       "      <td>0.006180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cfk</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.923854</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.967722</td>\n",
       "      <td>8.778483e-03</td>\n",
       "      <td>0.937581</td>\n",
       "      <td>1.222529e-03</td>\n",
       "      <td>0.959103</td>\n",
       "      <td>0.009163</td>\n",
       "      <td>4.647323</td>\n",
       "      <td>6.567608</td>\n",
       "      <td>0.031657</td>\n",
       "      <td>2.633545e-03</td>\n",
       "      <td>0.542481</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.042481</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.919447</td>\n",
       "      <td>0.001028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cfk</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.922876</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>0.969584</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>0.936927</td>\n",
       "      <td>1.481265e-03</td>\n",
       "      <td>0.955537</td>\n",
       "      <td>0.007862</td>\n",
       "      <td>5.808185</td>\n",
       "      <td>8.209316</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>5.238750e-17</td>\n",
       "      <td>0.544264</td>\n",
       "      <td>0.004824</td>\n",
       "      <td>0.044264</td>\n",
       "      <td>0.004824</td>\n",
       "      <td>0.917174</td>\n",
       "      <td>0.006059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FisherForgetting</td>\n",
       "      <td>1.00E-07</td>\n",
       "      <td>0.851990</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>0.881440</td>\n",
       "      <td>2.984684e-02</td>\n",
       "      <td>0.856818</td>\n",
       "      <td>3.645594e-02</td>\n",
       "      <td>0.902777</td>\n",
       "      <td>0.034580</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>0.156613</td>\n",
       "      <td>0.054624</td>\n",
       "      <td>3.599178e-02</td>\n",
       "      <td>0.536357</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.036357</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.889638</td>\n",
       "      <td>0.014001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FisherForgetting</td>\n",
       "      <td>1.00E-08</td>\n",
       "      <td>0.819717</td>\n",
       "      <td>0.062705</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>6.583862e-02</td>\n",
       "      <td>0.824126</td>\n",
       "      <td>6.560258e-02</td>\n",
       "      <td>0.866416</td>\n",
       "      <td>0.055403</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>0.156613</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>5.267090e-02</td>\n",
       "      <td>0.529845</td>\n",
       "      <td>0.008990</td>\n",
       "      <td>0.029845</td>\n",
       "      <td>0.008990</td>\n",
       "      <td>0.880014</td>\n",
       "      <td>0.022363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FisherForgetting</td>\n",
       "      <td>1.00E-06</td>\n",
       "      <td>0.768472</td>\n",
       "      <td>0.080234</td>\n",
       "      <td>0.829299</td>\n",
       "      <td>6.408292e-02</td>\n",
       "      <td>0.772053</td>\n",
       "      <td>8.395991e-02</td>\n",
       "      <td>0.798519</td>\n",
       "      <td>0.086343</td>\n",
       "      <td>0.612679</td>\n",
       "      <td>0.077917</td>\n",
       "      <td>0.106766</td>\n",
       "      <td>7.022786e-02</td>\n",
       "      <td>0.546589</td>\n",
       "      <td>0.010086</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.010086</td>\n",
       "      <td>0.837647</td>\n",
       "      <td>0.050203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>eu_k</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.923983</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.970826</td>\n",
       "      <td>7.022786e-03</td>\n",
       "      <td>0.938113</td>\n",
       "      <td>1.064053e-03</td>\n",
       "      <td>0.956468</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>0.501386</td>\n",
       "      <td>0.704368</td>\n",
       "      <td>0.034761</td>\n",
       "      <td>8.778483e-04</td>\n",
       "      <td>0.524264</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.024264</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.937728</td>\n",
       "      <td>0.005156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Identity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.922775</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.970205</td>\n",
       "      <td>5.267090e-03</td>\n",
       "      <td>0.936366</td>\n",
       "      <td>1.824091e-03</td>\n",
       "      <td>0.954771</td>\n",
       "      <td>0.007138</td>\n",
       "      <td>0.003873</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>8.778483e-04</td>\n",
       "      <td>0.544419</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>0.044419</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>0.916969</td>\n",
       "      <td>0.007839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NegGrad</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.922812</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>0.970205</td>\n",
       "      <td>5.267090e-03</td>\n",
       "      <td>0.936346</td>\n",
       "      <td>1.843496e-03</td>\n",
       "      <td>0.954837</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.003873</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>8.778483e-04</td>\n",
       "      <td>0.542016</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.042016</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.919390</td>\n",
       "      <td>0.006910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NegGrad</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.923141</td>\n",
       "      <td>0.002846</td>\n",
       "      <td>0.963998</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>0.936531</td>\n",
       "      <td>2.027845e-03</td>\n",
       "      <td>0.961174</td>\n",
       "      <td>0.008666</td>\n",
       "      <td>1.718996</td>\n",
       "      <td>0.546976</td>\n",
       "      <td>0.027933</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.542713</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.042713</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.918857</td>\n",
       "      <td>0.004164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NegGrad</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.785989</td>\n",
       "      <td>0.042083</td>\n",
       "      <td>0.752948</td>\n",
       "      <td>3.072469e-02</td>\n",
       "      <td>0.790216</td>\n",
       "      <td>4.263650e-02</td>\n",
       "      <td>0.835228</td>\n",
       "      <td>0.034262</td>\n",
       "      <td>1.774643</td>\n",
       "      <td>0.155834</td>\n",
       "      <td>0.183116</td>\n",
       "      <td>2.457975e-02</td>\n",
       "      <td>0.497984</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.890979</td>\n",
       "      <td>0.020603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Scrub</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.921220</td>\n",
       "      <td>0.003351</td>\n",
       "      <td>0.955928</td>\n",
       "      <td>1.755697e-03</td>\n",
       "      <td>0.941006</td>\n",
       "      <td>4.447029e-03</td>\n",
       "      <td>0.964981</td>\n",
       "      <td>0.005582</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019863</td>\n",
       "      <td>4.389241e-03</td>\n",
       "      <td>0.539380</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.039380</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.921230</td>\n",
       "      <td>0.004526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Scrub</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.919006</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0.944134</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.929357</td>\n",
       "      <td>6.293759e-03</td>\n",
       "      <td>0.971873</td>\n",
       "      <td>0.012296</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>0.156613</td>\n",
       "      <td>0.008070</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>0.529380</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.029380</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.930123</td>\n",
       "      <td>0.003664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Scrub</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.519443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.536313</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.517639</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.586770</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>12.567302</td>\n",
       "      <td>1.495224</td>\n",
       "      <td>0.399752</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>0.502171</td>\n",
       "      <td>0.005043</td>\n",
       "      <td>0.004031</td>\n",
       "      <td>0.003727</td>\n",
       "      <td>0.755691</td>\n",
       "      <td>0.003727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ssd</td>\n",
       "      <td>0.1_20</td>\n",
       "      <td>0.918942</td>\n",
       "      <td>0.004631</td>\n",
       "      <td>0.966480</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.931356</td>\n",
       "      <td>4.935394e-03</td>\n",
       "      <td>0.950988</td>\n",
       "      <td>0.006237</td>\n",
       "      <td>0.889809</td>\n",
       "      <td>0.625672</td>\n",
       "      <td>0.030416</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>0.542713</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.042713</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.916758</td>\n",
       "      <td>0.000123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ssd</td>\n",
       "      <td>0.01_20</td>\n",
       "      <td>0.918942</td>\n",
       "      <td>0.004631</td>\n",
       "      <td>0.966480</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.931356</td>\n",
       "      <td>4.935394e-03</td>\n",
       "      <td>0.950988</td>\n",
       "      <td>0.006237</td>\n",
       "      <td>0.889809</td>\n",
       "      <td>0.625672</td>\n",
       "      <td>0.030416</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>0.548217</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.048217</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.911254</td>\n",
       "      <td>0.002864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ssd</td>\n",
       "      <td>0.1_40</td>\n",
       "      <td>0.918933</td>\n",
       "      <td>0.004618</td>\n",
       "      <td>0.966480</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.931356</td>\n",
       "      <td>4.935394e-03</td>\n",
       "      <td>0.950970</td>\n",
       "      <td>0.006213</td>\n",
       "      <td>0.889809</td>\n",
       "      <td>0.625672</td>\n",
       "      <td>0.030416</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>0.544961</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.044961</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.914505</td>\n",
       "      <td>0.005160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ssd</td>\n",
       "      <td>0.01_40</td>\n",
       "      <td>0.918933</td>\n",
       "      <td>0.004618</td>\n",
       "      <td>0.966480</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.931356</td>\n",
       "      <td>4.935394e-03</td>\n",
       "      <td>0.950970</td>\n",
       "      <td>0.006213</td>\n",
       "      <td>0.889809</td>\n",
       "      <td>0.625672</td>\n",
       "      <td>0.030416</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>0.546977</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>0.046977</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>0.912490</td>\n",
       "      <td>0.004283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>unsir</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.924248</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.934823</td>\n",
       "      <td>7.900634e-03</td>\n",
       "      <td>0.927159</td>\n",
       "      <td>8.538296e-04</td>\n",
       "      <td>0.991061</td>\n",
       "      <td>0.009304</td>\n",
       "      <td>1.165837</td>\n",
       "      <td>0.234530</td>\n",
       "      <td>0.013656</td>\n",
       "      <td>3.511393e-03</td>\n",
       "      <td>0.510775</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.010775</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.951349</td>\n",
       "      <td>0.000870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>unsir</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.924842</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>5.267090e-03</td>\n",
       "      <td>0.927136</td>\n",
       "      <td>6.630117e-04</td>\n",
       "      <td>0.988528</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>1.165837</td>\n",
       "      <td>0.234530</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>8.778483e-04</td>\n",
       "      <td>0.515736</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>0.015736</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>0.946685</td>\n",
       "      <td>0.002071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>unsir</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.924495</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.939168</td>\n",
       "      <td>1.755697e-03</td>\n",
       "      <td>0.927223</td>\n",
       "      <td>1.778812e-04</td>\n",
       "      <td>0.987231</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>1.165837</td>\n",
       "      <td>0.234530</td>\n",
       "      <td>0.003104</td>\n",
       "      <td>4.389241e-03</td>\n",
       "      <td>0.520233</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.020233</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.942015</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>SuccessiveRandomLabels</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.921504</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.939789</td>\n",
       "      <td>8.778483e-04</td>\n",
       "      <td>0.924527</td>\n",
       "      <td>3.234203e-04</td>\n",
       "      <td>0.980793</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>1.387321</td>\n",
       "      <td>0.077917</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>5.267090e-03</td>\n",
       "      <td>0.503953</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.003953</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.956798</td>\n",
       "      <td>0.000741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>SuccessiveRandomLabels</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.922345</td>\n",
       "      <td>0.001992</td>\n",
       "      <td>0.946617</td>\n",
       "      <td>8.778483e-04</td>\n",
       "      <td>0.936449</td>\n",
       "      <td>2.464463e-03</td>\n",
       "      <td>0.975884</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>0.156613</td>\n",
       "      <td>0.010552</td>\n",
       "      <td>7.022786e-03</td>\n",
       "      <td>0.526202</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.026202</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.934971</td>\n",
       "      <td>0.000210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>SuccessiveRandomLabels</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.923946</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.950962</td>\n",
       "      <td>8.778483e-04</td>\n",
       "      <td>0.949496</td>\n",
       "      <td>7.018221e-04</td>\n",
       "      <td>0.974833</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014898</td>\n",
       "      <td>5.267090e-03</td>\n",
       "      <td>0.520310</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>0.941663</td>\n",
       "      <td>0.003405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>salun</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.923397</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.951583</td>\n",
       "      <td>2.633545e-03</td>\n",
       "      <td>0.938267</td>\n",
       "      <td>6.306696e-04</td>\n",
       "      <td>0.973192</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>0.156613</td>\n",
       "      <td>0.015518</td>\n",
       "      <td>3.511393e-03</td>\n",
       "      <td>0.523798</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>0.023798</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>0.937900</td>\n",
       "      <td>0.003192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>salun</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.922583</td>\n",
       "      <td>0.001514</td>\n",
       "      <td>0.933582</td>\n",
       "      <td>8.778483e-04</td>\n",
       "      <td>0.925416</td>\n",
       "      <td>2.357734e-03</td>\n",
       "      <td>0.988946</td>\n",
       "      <td>0.006321</td>\n",
       "      <td>2.880961</td>\n",
       "      <td>0.780726</td>\n",
       "      <td>0.007449</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.512403</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.012403</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.948888</td>\n",
       "      <td>0.003498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>salun</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.924367</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.949721</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.949498</td>\n",
       "      <td>7.276957e-04</td>\n",
       "      <td>0.976825</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013656</td>\n",
       "      <td>6.144938e-03</td>\n",
       "      <td>0.526512</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.026512</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.935672</td>\n",
       "      <td>0.001084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 unlearner        lr  \\\n",
       "0                GoldModel       NaN   \n",
       "1          AdvancedNegGrad    0.0001   \n",
       "2          AdvancedNegGrad   0.00001   \n",
       "3              BadTeaching     0.001   \n",
       "4              BadTeaching    0.0001   \n",
       "5              BadTeaching   0.00001   \n",
       "6               Finetuning     0.001   \n",
       "7               Finetuning    0.0001   \n",
       "8               Finetuning   0.00001   \n",
       "9                      cfk     0.001   \n",
       "10                     cfk    0.0001   \n",
       "11                     cfk   0.00001   \n",
       "12        FisherForgetting  1.00E-07   \n",
       "13        FisherForgetting  1.00E-08   \n",
       "14        FisherForgetting  1.00E-06   \n",
       "15                    eu_k      0.01   \n",
       "16                Identity       NaN   \n",
       "17                 NegGrad   0.00001   \n",
       "18                 NegGrad    0.0001   \n",
       "19                 NegGrad     0.001   \n",
       "20                   Scrub    0.0001   \n",
       "21                   Scrub     0.001   \n",
       "22                   Scrub      0.01   \n",
       "23                     ssd    0.1_20   \n",
       "24                     ssd   0.01_20   \n",
       "25                     ssd    0.1_40   \n",
       "26                     ssd   0.01_40   \n",
       "27                   unsir     0.001   \n",
       "28                   unsir      0.01   \n",
       "29                   unsir    0.0001   \n",
       "30  SuccessiveRandomLabels      0.01   \n",
       "31  SuccessiveRandomLabels     0.001   \n",
       "32  SuccessiveRandomLabels    0.0001   \n",
       "33                   salun     0.001   \n",
       "34                   salun      0.01   \n",
       "35                   salun    0.0001   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.test.unlearned  \\\n",
       "0                                        0.924166   \n",
       "1                                        0.888901   \n",
       "2                                        0.731277   \n",
       "3                                        0.493519   \n",
       "4                                        0.480557   \n",
       "5                                        0.569609   \n",
       "6                                        0.922665   \n",
       "7                                        0.924101   \n",
       "8                                        0.927889   \n",
       "9                                        0.924028   \n",
       "10                                       0.923854   \n",
       "11                                       0.922876   \n",
       "12                                       0.851990   \n",
       "13                                       0.819717   \n",
       "14                                       0.768472   \n",
       "15                                       0.923983   \n",
       "16                                       0.922775   \n",
       "17                                       0.922812   \n",
       "18                                       0.923141   \n",
       "19                                       0.785989   \n",
       "20                                       0.921220   \n",
       "21                                       0.919006   \n",
       "22                                       0.519443   \n",
       "23                                       0.918942   \n",
       "24                                       0.918942   \n",
       "25                                       0.918933   \n",
       "26                                       0.918933   \n",
       "27                                       0.924248   \n",
       "28                                       0.924842   \n",
       "29                                       0.924495   \n",
       "30                                       0.921504   \n",
       "31                                       0.922345   \n",
       "32                                       0.923946   \n",
       "33                                       0.923397   \n",
       "34                                       0.922583   \n",
       "35                                       0.924367   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.test.unlearned_std  \\\n",
       "0                                            0.000362   \n",
       "1                                            0.014218   \n",
       "2                                            0.025033   \n",
       "3                                            0.018331   \n",
       "4                                            0.000000   \n",
       "5                                            0.030971   \n",
       "6                                            0.000970   \n",
       "7                                            0.000492   \n",
       "8                                            0.000556   \n",
       "9                                            0.001617   \n",
       "10                                           0.001837   \n",
       "11                                           0.002471   \n",
       "12                                           0.034140   \n",
       "13                                           0.062705   \n",
       "14                                           0.080234   \n",
       "15                                           0.001746   \n",
       "16                                           0.002523   \n",
       "17                                           0.002419   \n",
       "18                                           0.002846   \n",
       "19                                           0.042083   \n",
       "20                                           0.003351   \n",
       "21                                           0.005136   \n",
       "22                                           0.000000   \n",
       "23                                           0.004631   \n",
       "24                                           0.004631   \n",
       "25                                           0.004618   \n",
       "26                                           0.004618   \n",
       "27                                           0.000453   \n",
       "28                                           0.000414   \n",
       "29                                           0.000750   \n",
       "30                                           0.000492   \n",
       "31                                           0.001992   \n",
       "32                                           0.000207   \n",
       "33                                           0.000026   \n",
       "34                                           0.001514   \n",
       "35                                           0.000194   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.forget.unlearned  \\\n",
       "0                                          0.936065   \n",
       "1                                          0.032899   \n",
       "2                                          0.484171   \n",
       "3                                          0.487896   \n",
       "4                                          0.463687   \n",
       "5                                          0.601490   \n",
       "6                                          0.953445   \n",
       "7                                          0.954687   \n",
       "8                                          0.970826   \n",
       "9                                          0.968343   \n",
       "10                                         0.967722   \n",
       "11                                         0.969584   \n",
       "12                                         0.881440   \n",
       "13                                         0.854749   \n",
       "14                                         0.829299   \n",
       "15                                         0.970826   \n",
       "16                                         0.970205   \n",
       "17                                         0.970205   \n",
       "18                                         0.963998   \n",
       "19                                         0.752948   \n",
       "20                                         0.955928   \n",
       "21                                         0.944134   \n",
       "22                                         0.536313   \n",
       "23                                         0.966480   \n",
       "24                                         0.966480   \n",
       "25                                         0.966480   \n",
       "26                                         0.966480   \n",
       "27                                         0.934823   \n",
       "28                                         0.938547   \n",
       "29                                         0.939168   \n",
       "30                                         0.939789   \n",
       "31                                         0.946617   \n",
       "32                                         0.950962   \n",
       "33                                         0.951583   \n",
       "34                                         0.933582   \n",
       "35                                         0.949721   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.forget.unlearned_std  \\\n",
       "0                                        6.144938e-03     \n",
       "1                                        8.778483e-04     \n",
       "2                                        5.793799e-02     \n",
       "3                                        3.423608e-02     \n",
       "4                                        5.551115e-17     \n",
       "5                                        3.950317e-02     \n",
       "6                                        2.633545e-03     \n",
       "7                                        3.511393e-03     \n",
       "8                                        1.755697e-03     \n",
       "9                                        7.900634e-03     \n",
       "10                                       8.778483e-03     \n",
       "11                                       6.144938e-03     \n",
       "12                                       2.984684e-02     \n",
       "13                                       6.583862e-02     \n",
       "14                                       6.408292e-02     \n",
       "15                                       7.022786e-03     \n",
       "16                                       5.267090e-03     \n",
       "17                                       5.267090e-03     \n",
       "18                                       6.144938e-03     \n",
       "19                                       3.072469e-02     \n",
       "20                                       1.755697e-03     \n",
       "21                                       0.000000e+00     \n",
       "22                                       0.000000e+00     \n",
       "23                                       0.000000e+00     \n",
       "24                                       0.000000e+00     \n",
       "25                                       0.000000e+00     \n",
       "26                                       0.000000e+00     \n",
       "27                                       7.900634e-03     \n",
       "28                                       5.267090e-03     \n",
       "29                                       1.755697e-03     \n",
       "30                                       8.778483e-04     \n",
       "31                                       8.778483e-04     \n",
       "32                                       8.778483e-04     \n",
       "33                                       2.633545e-03     \n",
       "34                                       8.778483e-04     \n",
       "35                                       0.000000e+00     \n",
       "\n",
       "    sklearn.metrics.accuracy_score.retain.unlearned  \\\n",
       "0                                          0.937864   \n",
       "1                                          0.909467   \n",
       "2                                          0.748774   \n",
       "3                                          0.494120   \n",
       "4                                          0.482361   \n",
       "5                                          0.570408   \n",
       "6                                          0.937631   \n",
       "7                                          0.949752   \n",
       "8                                          0.945004   \n",
       "9                                          0.937905   \n",
       "10                                         0.937581   \n",
       "11                                         0.936927   \n",
       "12                                         0.856818   \n",
       "13                                         0.824126   \n",
       "14                                         0.772053   \n",
       "15                                         0.938113   \n",
       "16                                         0.936366   \n",
       "17                                         0.936346   \n",
       "18                                         0.936531   \n",
       "19                                         0.790216   \n",
       "20                                         0.941006   \n",
       "21                                         0.929357   \n",
       "22                                         0.517639   \n",
       "23                                         0.931356   \n",
       "24                                         0.931356   \n",
       "25                                         0.931356   \n",
       "26                                         0.931356   \n",
       "27                                         0.927159   \n",
       "28                                         0.927136   \n",
       "29                                         0.927223   \n",
       "30                                         0.924527   \n",
       "31                                         0.936449   \n",
       "32                                         0.949496   \n",
       "33                                         0.938267   \n",
       "34                                         0.925416   \n",
       "35                                         0.949498   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.retain.unlearned_std       AUS   AUS_std  \\\n",
       "0                                        1.487733e-04    0.989631  0.002816   \n",
       "1                                        1.182425e-02    0.520503  0.005305   \n",
       "2                                        2.733872e-02    0.649319  0.038509   \n",
       "3                                        1.663027e-02    0.561275  0.015547   \n",
       "4                                        5.551115e-17    0.548528  0.002481   \n",
       "5                                        3.298564e-02    0.626623  0.027438   \n",
       "6                                        1.041413e-03    0.970039  0.001890   \n",
       "7                                        7.956140e-04    0.971609  0.000078   \n",
       "8                                        4.269148e-04    0.963734  0.000251   \n",
       "9                                        1.109332e-03    0.958838  0.007923   \n",
       "10                                       1.222529e-03    0.959103  0.009163   \n",
       "11                                       1.481265e-03    0.955537  0.007862   \n",
       "12                                       3.645594e-02    0.902777  0.034580   \n",
       "13                                       6.560258e-02    0.866416  0.055403   \n",
       "14                                       8.395991e-02    0.798519  0.086343   \n",
       "15                                       1.064053e-03    0.956468  0.007314   \n",
       "16                                       1.824091e-03    0.954771  0.007138   \n",
       "17                                       1.843496e-03    0.954837  0.006944   \n",
       "18                                       2.027845e-03    0.961174  0.008666   \n",
       "19                                       4.263650e-02    0.835228  0.034262   \n",
       "20                                       4.447029e-03    0.964981  0.005582   \n",
       "21                                       6.293759e-03    0.971873  0.012296   \n",
       "22                                       0.000000e+00    0.586770  0.002481   \n",
       "23                                       4.935394e-03    0.950988  0.006237   \n",
       "24                                       4.935394e-03    0.950988  0.006237   \n",
       "25                                       4.935394e-03    0.950970  0.006213   \n",
       "26                                       4.935394e-03    0.950970  0.006213   \n",
       "27                                       8.538296e-04    0.991061  0.009304   \n",
       "28                                       6.630117e-04    0.988528  0.001842   \n",
       "29                                       1.778812e-04    0.987231  0.002249   \n",
       "30                                       3.234203e-04    0.980793  0.000676   \n",
       "31                                       2.464463e-03    0.975884  0.000544   \n",
       "32                                       7.018221e-04    0.974833  0.002022   \n",
       "33                                       6.306696e-04    0.973192  0.000089   \n",
       "34                                       2.357734e-03    0.988946  0.006321   \n",
       "35                                       7.276957e-04    0.976825  0.002086   \n",
       "\n",
       "          AIN   AIN_std  forget_diff  forget_diff_std      UMIA  UMIA_std  \\\n",
       "0    0.778516  0.156613     0.000000     0.000000e+00  0.508837  0.000000   \n",
       "1    1.553159  0.312446     0.903166     5.267090e-03  0.971628  0.003289   \n",
       "2   38.736015  7.792461     0.451893     5.179305e-02  0.635659  0.019185   \n",
       "3    1.553159  0.312446     0.448169     4.038102e-02  0.512868  0.007455   \n",
       "4    1.553159  0.312446     0.472377     6.144938e-03  0.504264  0.009428   \n",
       "5    6.640678  5.158093     0.334575     3.335823e-02  0.517287  0.000767   \n",
       "6    0.778516  0.156613     0.017381     3.511393e-03  0.534109  0.004056   \n",
       "7    1.000000  0.000000     0.018622     2.633545e-03  0.529690  0.002412   \n",
       "8    0.446841  0.312446     0.034761     4.389241e-03  0.533643  0.000548   \n",
       "9    2.491435  3.518723     0.032278     1.755697e-03  0.544806  0.005372   \n",
       "10   4.647323  6.567608     0.031657     2.633545e-03  0.542481  0.000110   \n",
       "11   5.808185  8.209316     0.033520     5.238750e-17  0.544264  0.004824   \n",
       "12   0.778516  0.156613     0.054624     3.599178e-02  0.536357  0.003070   \n",
       "13   0.778516  0.156613     0.094972     5.267090e-02  0.529845  0.008990   \n",
       "14   0.612679  0.077917     0.106766     7.022786e-02  0.546589  0.010086   \n",
       "15   0.501386  0.704368     0.034761     8.778483e-04  0.524264  0.006030   \n",
       "16   0.003873  0.000779     0.034140     8.778483e-04  0.544419  0.006578   \n",
       "17   0.003873  0.000779     0.034140     8.778483e-04  0.542016  0.005701   \n",
       "18   1.718996  0.546976     0.027933     0.000000e+00  0.542713  0.002741   \n",
       "19   1.774643  0.155834     0.183116     2.457975e-02  0.497984  0.000439   \n",
       "20   1.000000  0.000000     0.019863     4.389241e-03  0.539380  0.002850   \n",
       "21   0.778516  0.156613     0.008070     6.144938e-03  0.529380  0.001096   \n",
       "22  12.567302  1.495224     0.399752     6.144938e-03  0.502171  0.005043   \n",
       "23   0.889809  0.625672     0.030416     6.144938e-03  0.542713  0.002193   \n",
       "24   0.889809  0.625672     0.030416     6.144938e-03  0.548217  0.000548   \n",
       "25   0.889809  0.625672     0.030416     6.144938e-03  0.544961  0.002850   \n",
       "26   0.889809  0.625672     0.030416     6.144938e-03  0.546977  0.001973   \n",
       "27   1.165837  0.234530     0.013656     3.511393e-03  0.510775  0.001096   \n",
       "28   1.165837  0.234530     0.002483     8.778483e-04  0.515736  0.001864   \n",
       "29   1.165837  0.234530     0.003104     4.389241e-03  0.520233  0.000329   \n",
       "30   1.387321  0.077917     0.003724     5.267090e-03  0.503953  0.000987   \n",
       "31   0.778516  0.156613     0.010552     7.022786e-03  0.526202  0.001206   \n",
       "32   1.000000  0.000000     0.014898     5.267090e-03  0.520310  0.003508   \n",
       "33   0.778516  0.156613     0.015518     3.511393e-03  0.523798  0.003179   \n",
       "34   2.880961  0.780726     0.007449     0.000000e+00  0.512403  0.002741   \n",
       "35   1.000000  0.000000     0.013656     6.144938e-03  0.526512  0.000987   \n",
       "\n",
       "    Unlearning_Score  Unlearning_Score_std     nomus  nomus_std  \n",
       "0           0.008837              0.000000  0.953246   0.000181  \n",
       "1           0.471628              0.003289  0.472823   0.003820  \n",
       "2           0.135659              0.019185  0.729980   0.031701  \n",
       "3           0.012868              0.007455  0.733891   0.016621  \n",
       "4           0.010310              0.000877  0.729968   0.000877  \n",
       "5           0.017287              0.000767  0.767518   0.016253  \n",
       "6           0.034109              0.004056  0.927224   0.004541  \n",
       "7           0.029690              0.002412  0.932361   0.002166  \n",
       "8           0.033643              0.000548  0.930301   0.000270  \n",
       "9           0.044806              0.005372  0.917208   0.006180  \n",
       "10          0.042481              0.000110  0.919447   0.001028  \n",
       "11          0.044264              0.004824  0.917174   0.006059  \n",
       "12          0.036357              0.003070  0.889638   0.014001  \n",
       "13          0.029845              0.008990  0.880014   0.022363  \n",
       "14          0.046589              0.010086  0.837647   0.050203  \n",
       "15          0.024264              0.006030  0.937728   0.005156  \n",
       "16          0.044419              0.006578  0.916969   0.007839  \n",
       "17          0.042016              0.005701  0.919390   0.006910  \n",
       "18          0.042713              0.002741  0.918857   0.004164  \n",
       "19          0.002016              0.000439  0.890979   0.020603  \n",
       "20          0.039380              0.002850  0.921230   0.004526  \n",
       "21          0.029380              0.001096  0.930123   0.003664  \n",
       "22          0.004031              0.003727  0.755691   0.003727  \n",
       "23          0.042713              0.002193  0.916758   0.000123  \n",
       "24          0.048217              0.000548  0.911254   0.002864  \n",
       "25          0.044961              0.002850  0.914505   0.005160  \n",
       "26          0.046977              0.001973  0.912490   0.004283  \n",
       "27          0.010775              0.001096  0.951349   0.000870  \n",
       "28          0.015736              0.001864  0.946685   0.002071  \n",
       "29          0.020233              0.000329  0.942015   0.000046  \n",
       "30          0.003953              0.000987  0.956798   0.000741  \n",
       "31          0.026202              0.001206  0.934971   0.000210  \n",
       "32          0.020310              0.003508  0.941663   0.003405  \n",
       "33          0.023798              0.003179  0.937900   0.003192  \n",
       "34          0.012403              0.002741  0.948888   0.003498  \n",
       "35          0.026512              0.000987  0.935672   0.001084  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df\n",
    "\n",
    "# remove index 1 and 2 \n",
    "\n",
    "# change unleraner of index 11, 12, 13 to cfk \n",
    "new_df['unlearner'][9] = \"cfk\"\n",
    "new_df['unlearner'][10] = \"cfk\"\n",
    "new_df['unlearner'][11] = \"cfk\"\n",
    "\n",
    "# change lr to number 14,15,16 to 1.00E-07, 1.00E-08, 1.00E-06\n",
    "new_df['lr'][12] = '1.00E-07'\n",
    "new_df['lr'][13] = '1.00E-08'\n",
    "new_df['lr'][14] = '1.00E-06'\n",
    "\n",
    "# change number 29, 30, 31 to unsir \n",
    "new_df['unlearner'][27] = 'unsir'\n",
    "new_df['unlearner'][28] = 'unsir'\n",
    "new_df['unlearner'][29] = 'unsir'\n",
    "new_df['lr'][27] = 0.001\n",
    "new_df['lr'][28] = 0.01\n",
    "new_df['lr'][29] = 0.0001\n",
    "\n",
    "# change number 35,36,37 to salun\n",
    "new_df['unlearner'][33] = 'salun'\n",
    "new_df['unlearner'][34] = 'salun'\n",
    "new_df['unlearner'][35] = 'salun'\n",
    "new_df['lr'][33] = 0.001\n",
    "new_df['lr'][34] = 0.01\n",
    "new_df['lr'][35] = 0.0001\n",
    "\n",
    "new_df['unlearner'][23] = \"ssd\"\n",
    "new_df['unlearner'][24] = \"ssd\"\n",
    "new_df['unlearner'][25] = \"ssd\"\n",
    "new_df['unlearner'][26] = \"ssd\"\n",
    "new_df['lr'][23] = \"0.1_20\"\n",
    "new_df['lr'][24] = \"0.01_20\"\n",
    "new_df['lr'][25] = \"0.1_40\"\n",
    "new_df['lr'][26] = \"0.01_40\"\n",
    "\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unlearner</th>\n",
       "      <th>lr</th>\n",
       "      <th>sklearn.metrics.accuracy_score.test.unlearned</th>\n",
       "      <th>sklearn.metrics.accuracy_score.test.unlearned_std</th>\n",
       "      <th>sklearn.metrics.accuracy_score.forget.unlearned</th>\n",
       "      <th>sklearn.metrics.accuracy_score.forget.unlearned_std</th>\n",
       "      <th>sklearn.metrics.accuracy_score.retain.unlearned</th>\n",
       "      <th>sklearn.metrics.accuracy_score.retain.unlearned_std</th>\n",
       "      <th>AUS</th>\n",
       "      <th>AUS_std</th>\n",
       "      <th>AIN</th>\n",
       "      <th>AIN_std</th>\n",
       "      <th>forget_diff</th>\n",
       "      <th>forget_diff_std</th>\n",
       "      <th>UMIA</th>\n",
       "      <th>UMIA_std</th>\n",
       "      <th>Unlearning_Score</th>\n",
       "      <th>Unlearning_Score_std</th>\n",
       "      <th>nomus</th>\n",
       "      <th>nomus_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GoldModel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.924166</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.936065</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.937864</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.989631</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>0.156613</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.508837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.953246</td>\n",
       "      <td>0.000181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdvancedNegGrad</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.731277</td>\n",
       "      <td>0.025033</td>\n",
       "      <td>0.484171</td>\n",
       "      <td>0.057938</td>\n",
       "      <td>0.748774</td>\n",
       "      <td>0.027339</td>\n",
       "      <td>0.649319</td>\n",
       "      <td>0.038509</td>\n",
       "      <td>38.736015</td>\n",
       "      <td>7.792461</td>\n",
       "      <td>0.451893</td>\n",
       "      <td>0.051793</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>0.019185</td>\n",
       "      <td>0.135659</td>\n",
       "      <td>0.019185</td>\n",
       "      <td>0.729980</td>\n",
       "      <td>0.031701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BadTeaching</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.569609</td>\n",
       "      <td>0.030971</td>\n",
       "      <td>0.601490</td>\n",
       "      <td>0.039503</td>\n",
       "      <td>0.570408</td>\n",
       "      <td>0.032986</td>\n",
       "      <td>0.626623</td>\n",
       "      <td>0.027438</td>\n",
       "      <td>6.640678</td>\n",
       "      <td>5.158093</td>\n",
       "      <td>0.334575</td>\n",
       "      <td>0.033358</td>\n",
       "      <td>0.517287</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.017287</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.767518</td>\n",
       "      <td>0.016253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Finetuning</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.924101</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.954687</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.949752</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.971609</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018622</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.529690</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.029690</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.932361</td>\n",
       "      <td>0.002166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cfk</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.923854</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.967722</td>\n",
       "      <td>0.008778</td>\n",
       "      <td>0.937581</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.959103</td>\n",
       "      <td>0.009163</td>\n",
       "      <td>4.647323</td>\n",
       "      <td>6.567608</td>\n",
       "      <td>0.031657</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.542481</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.042481</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.919447</td>\n",
       "      <td>0.001028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FisherForgetting</td>\n",
       "      <td>1.00E-07</td>\n",
       "      <td>0.851990</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>0.881440</td>\n",
       "      <td>0.029847</td>\n",
       "      <td>0.856818</td>\n",
       "      <td>0.036456</td>\n",
       "      <td>0.902777</td>\n",
       "      <td>0.034580</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>0.156613</td>\n",
       "      <td>0.054624</td>\n",
       "      <td>0.035992</td>\n",
       "      <td>0.536357</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.036357</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.889638</td>\n",
       "      <td>0.014001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>eu_k</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.923983</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.970826</td>\n",
       "      <td>0.007023</td>\n",
       "      <td>0.938113</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.956468</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>0.501386</td>\n",
       "      <td>0.704368</td>\n",
       "      <td>0.034761</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.524264</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.024264</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.937728</td>\n",
       "      <td>0.005156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Identity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.922775</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.970205</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>0.936366</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>0.954771</td>\n",
       "      <td>0.007138</td>\n",
       "      <td>0.003873</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.544419</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>0.044419</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>0.916969</td>\n",
       "      <td>0.007839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NegGrad</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.922812</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>0.970205</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>0.936346</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.954837</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.003873</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.542016</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.042016</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.919390</td>\n",
       "      <td>0.006910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Scrub</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.919006</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0.944134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.929357</td>\n",
       "      <td>0.006294</td>\n",
       "      <td>0.971873</td>\n",
       "      <td>0.012296</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>0.156613</td>\n",
       "      <td>0.008070</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.529380</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.029380</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.930123</td>\n",
       "      <td>0.003664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ssd</td>\n",
       "      <td>0.1_20</td>\n",
       "      <td>0.918942</td>\n",
       "      <td>0.004631</td>\n",
       "      <td>0.966480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.931356</td>\n",
       "      <td>0.004935</td>\n",
       "      <td>0.950988</td>\n",
       "      <td>0.006237</td>\n",
       "      <td>0.889809</td>\n",
       "      <td>0.625672</td>\n",
       "      <td>0.030416</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.542713</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.042713</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.916758</td>\n",
       "      <td>0.000123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>unsir</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.924248</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.934823</td>\n",
       "      <td>0.007901</td>\n",
       "      <td>0.927159</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.991061</td>\n",
       "      <td>0.009304</td>\n",
       "      <td>1.165837</td>\n",
       "      <td>0.234530</td>\n",
       "      <td>0.013656</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.510775</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.010775</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.951349</td>\n",
       "      <td>0.000870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>SuccessiveRandomLabels</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.921504</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.939789</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.924527</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.980793</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>1.387321</td>\n",
       "      <td>0.077917</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>0.503953</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.003953</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.956798</td>\n",
       "      <td>0.000741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>salun</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.922583</td>\n",
       "      <td>0.001514</td>\n",
       "      <td>0.933582</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.925416</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.988946</td>\n",
       "      <td>0.006321</td>\n",
       "      <td>2.880961</td>\n",
       "      <td>0.780726</td>\n",
       "      <td>0.007449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.512403</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.012403</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.948888</td>\n",
       "      <td>0.003498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 unlearner        lr  \\\n",
       "0                GoldModel       NaN   \n",
       "2          AdvancedNegGrad   0.00001   \n",
       "5              BadTeaching   0.00001   \n",
       "7               Finetuning    0.0001   \n",
       "10                     cfk    0.0001   \n",
       "12        FisherForgetting  1.00E-07   \n",
       "15                    eu_k      0.01   \n",
       "16                Identity       NaN   \n",
       "17                 NegGrad   0.00001   \n",
       "21                   Scrub     0.001   \n",
       "23                     ssd    0.1_20   \n",
       "27                   unsir     0.001   \n",
       "30  SuccessiveRandomLabels      0.01   \n",
       "34                   salun      0.01   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.test.unlearned  \\\n",
       "0                                        0.924166   \n",
       "2                                        0.731277   \n",
       "5                                        0.569609   \n",
       "7                                        0.924101   \n",
       "10                                       0.923854   \n",
       "12                                       0.851990   \n",
       "15                                       0.923983   \n",
       "16                                       0.922775   \n",
       "17                                       0.922812   \n",
       "21                                       0.919006   \n",
       "23                                       0.918942   \n",
       "27                                       0.924248   \n",
       "30                                       0.921504   \n",
       "34                                       0.922583   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.test.unlearned_std  \\\n",
       "0                                            0.000362   \n",
       "2                                            0.025033   \n",
       "5                                            0.030971   \n",
       "7                                            0.000492   \n",
       "10                                           0.001837   \n",
       "12                                           0.034140   \n",
       "15                                           0.001746   \n",
       "16                                           0.002523   \n",
       "17                                           0.002419   \n",
       "21                                           0.005136   \n",
       "23                                           0.004631   \n",
       "27                                           0.000453   \n",
       "30                                           0.000492   \n",
       "34                                           0.001514   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.forget.unlearned  \\\n",
       "0                                          0.936065   \n",
       "2                                          0.484171   \n",
       "5                                          0.601490   \n",
       "7                                          0.954687   \n",
       "10                                         0.967722   \n",
       "12                                         0.881440   \n",
       "15                                         0.970826   \n",
       "16                                         0.970205   \n",
       "17                                         0.970205   \n",
       "21                                         0.944134   \n",
       "23                                         0.966480   \n",
       "27                                         0.934823   \n",
       "30                                         0.939789   \n",
       "34                                         0.933582   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.forget.unlearned_std  \\\n",
       "0                                            0.006145     \n",
       "2                                            0.057938     \n",
       "5                                            0.039503     \n",
       "7                                            0.003511     \n",
       "10                                           0.008778     \n",
       "12                                           0.029847     \n",
       "15                                           0.007023     \n",
       "16                                           0.005267     \n",
       "17                                           0.005267     \n",
       "21                                           0.000000     \n",
       "23                                           0.000000     \n",
       "27                                           0.007901     \n",
       "30                                           0.000878     \n",
       "34                                           0.000878     \n",
       "\n",
       "    sklearn.metrics.accuracy_score.retain.unlearned  \\\n",
       "0                                          0.937864   \n",
       "2                                          0.748774   \n",
       "5                                          0.570408   \n",
       "7                                          0.949752   \n",
       "10                                         0.937581   \n",
       "12                                         0.856818   \n",
       "15                                         0.938113   \n",
       "16                                         0.936366   \n",
       "17                                         0.936346   \n",
       "21                                         0.929357   \n",
       "23                                         0.931356   \n",
       "27                                         0.927159   \n",
       "30                                         0.924527   \n",
       "34                                         0.925416   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.retain.unlearned_std       AUS   AUS_std  \\\n",
       "0                                            0.000149    0.989631  0.002816   \n",
       "2                                            0.027339    0.649319  0.038509   \n",
       "5                                            0.032986    0.626623  0.027438   \n",
       "7                                            0.000796    0.971609  0.000078   \n",
       "10                                           0.001223    0.959103  0.009163   \n",
       "12                                           0.036456    0.902777  0.034580   \n",
       "15                                           0.001064    0.956468  0.007314   \n",
       "16                                           0.001824    0.954771  0.007138   \n",
       "17                                           0.001843    0.954837  0.006944   \n",
       "21                                           0.006294    0.971873  0.012296   \n",
       "23                                           0.004935    0.950988  0.006237   \n",
       "27                                           0.000854    0.991061  0.009304   \n",
       "30                                           0.000323    0.980793  0.000676   \n",
       "34                                           0.002358    0.988946  0.006321   \n",
       "\n",
       "          AIN   AIN_std  forget_diff  forget_diff_std      UMIA  UMIA_std  \\\n",
       "0    0.778516  0.156613     0.000000         0.000000  0.508837  0.000000   \n",
       "2   38.736015  7.792461     0.451893         0.051793  0.635659  0.019185   \n",
       "5    6.640678  5.158093     0.334575         0.033358  0.517287  0.000767   \n",
       "7    1.000000  0.000000     0.018622         0.002634  0.529690  0.002412   \n",
       "10   4.647323  6.567608     0.031657         0.002634  0.542481  0.000110   \n",
       "12   0.778516  0.156613     0.054624         0.035992  0.536357  0.003070   \n",
       "15   0.501386  0.704368     0.034761         0.000878  0.524264  0.006030   \n",
       "16   0.003873  0.000779     0.034140         0.000878  0.544419  0.006578   \n",
       "17   0.003873  0.000779     0.034140         0.000878  0.542016  0.005701   \n",
       "21   0.778516  0.156613     0.008070         0.006145  0.529380  0.001096   \n",
       "23   0.889809  0.625672     0.030416         0.006145  0.542713  0.002193   \n",
       "27   1.165837  0.234530     0.013656         0.003511  0.510775  0.001096   \n",
       "30   1.387321  0.077917     0.003724         0.005267  0.503953  0.000987   \n",
       "34   2.880961  0.780726     0.007449         0.000000  0.512403  0.002741   \n",
       "\n",
       "    Unlearning_Score  Unlearning_Score_std     nomus  nomus_std  \n",
       "0           0.008837              0.000000  0.953246   0.000181  \n",
       "2           0.135659              0.019185  0.729980   0.031701  \n",
       "5           0.017287              0.000767  0.767518   0.016253  \n",
       "7           0.029690              0.002412  0.932361   0.002166  \n",
       "10          0.042481              0.000110  0.919447   0.001028  \n",
       "12          0.036357              0.003070  0.889638   0.014001  \n",
       "15          0.024264              0.006030  0.937728   0.005156  \n",
       "16          0.044419              0.006578  0.916969   0.007839  \n",
       "17          0.042016              0.005701  0.919390   0.006910  \n",
       "21          0.029380              0.001096  0.930123   0.003664  \n",
       "23          0.042713              0.002193  0.916758   0.000123  \n",
       "27          0.010775              0.001096  0.951349   0.000870  \n",
       "30          0.003953              0.000987  0.956798   0.000741  \n",
       "34          0.012403              0.002741  0.948888   0.003498  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new dataset that, for each unlearner, has the one with the highest nomus score\n",
    "\n",
    "best = {}\n",
    "\n",
    "for i, row in new_df.iterrows():\n",
    "    if row['unlearner'] not in best:\n",
    "        best[row['unlearner']] = row\n",
    "    else:\n",
    "        if row['nomus'] > best[row['unlearner']]['nomus']:\n",
    "            best[row['unlearner']] = row\n",
    "\n",
    "best_df = pd.DataFrame(list(best.values()))\n",
    "best_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GoldModel': [1594.3165845870972, 1617.2314856052399, 1584.8357911109924],\n",
       " 'AdvancedNegGrad': [276.5432913303375, 276.9710681438446, 279.06874442100525],\n",
       " 'BadTeaching': [155.62881207466125, 158.64404249191284, 155.7620885372162],\n",
       " 'Finetuning': [43.43743848800659, 44.081308364868164, 43.23821139335632],\n",
       " 'cfk': [35.66342067718506, 35.816524267196655, 35.06626892089844],\n",
       " 'FisherForgetting': [1816.6282725334167,\n",
       "  1712.6152826690675,\n",
       "  1788.850764541626],\n",
       " 'eu_k': [176.16479349136353, 166.61287879943848, 166.23280429840088],\n",
       " 'NegGrad': [0.3463304042816162, 0.32715773582458496, 0.32717037200927734],\n",
       " 'Scrub': [68.29331278800964, 64.5500967502594, 65.13795256614685],\n",
       " 'ssd': [101.25245833396912, 101.70019435882568, 103.1240565776825],\n",
       " 'unsir': [43.0055150986, 43.1281797886, 42.9365627766],\n",
       " 'SuccessiveRandomLabels': [40.14802575111389,\n",
       "  40.88470125198364,\n",
       "  40.284966707229614],\n",
       " 'salun': [41.04909634590149, 42.7211003304, 42.5594079494]}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read txt file with the times, where there is a line for each unlearner as \"name time\", for three times for the evaluation of mean and std \n",
    "times = {}\n",
    "\n",
    "with open(f\"output/runs/survey/time/{dataset_name}.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        name, time = line.split(\":\")\n",
    "        if name not in times:\n",
    "            times[name] = []\n",
    "        times[name].append(float(time))\n",
    "\n",
    "times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_316471/1241502917.py:10: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1598.7946204344432' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  best_df.loc[i, 'time'] = np.mean(times[name])\n",
      "/tmp/ipykernel_316471/1241502917.py:11: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '13.599261235541196' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  best_df.loc[i, 'time_std'] = np.std(times[name])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unlearner</th>\n",
       "      <th>lr</th>\n",
       "      <th>sklearn.metrics.accuracy_score.test.unlearned</th>\n",
       "      <th>sklearn.metrics.accuracy_score.test.unlearned_std</th>\n",
       "      <th>sklearn.metrics.accuracy_score.forget.unlearned</th>\n",
       "      <th>sklearn.metrics.accuracy_score.forget.unlearned_std</th>\n",
       "      <th>sklearn.metrics.accuracy_score.retain.unlearned</th>\n",
       "      <th>sklearn.metrics.accuracy_score.retain.unlearned_std</th>\n",
       "      <th>AUS</th>\n",
       "      <th>AUS_std</th>\n",
       "      <th>...</th>\n",
       "      <th>forget_diff</th>\n",
       "      <th>forget_diff_std</th>\n",
       "      <th>UMIA</th>\n",
       "      <th>UMIA_std</th>\n",
       "      <th>Unlearning_Score</th>\n",
       "      <th>Unlearning_Score_std</th>\n",
       "      <th>nomus</th>\n",
       "      <th>nomus_std</th>\n",
       "      <th>time</th>\n",
       "      <th>time_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GoldModel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.924166</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.936065</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.937864</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.989631</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.508837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.953246</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>1598.794620</td>\n",
       "      <td>13.599261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdvancedNegGrad</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.731277</td>\n",
       "      <td>0.025033</td>\n",
       "      <td>0.484171</td>\n",
       "      <td>0.057938</td>\n",
       "      <td>0.748774</td>\n",
       "      <td>0.027339</td>\n",
       "      <td>0.649319</td>\n",
       "      <td>0.038509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.451893</td>\n",
       "      <td>0.051793</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>0.019185</td>\n",
       "      <td>0.135659</td>\n",
       "      <td>0.019185</td>\n",
       "      <td>0.729980</td>\n",
       "      <td>0.031701</td>\n",
       "      <td>277.527701</td>\n",
       "      <td>1.103588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BadTeaching</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.569609</td>\n",
       "      <td>0.030971</td>\n",
       "      <td>0.601490</td>\n",
       "      <td>0.039503</td>\n",
       "      <td>0.570408</td>\n",
       "      <td>0.032986</td>\n",
       "      <td>0.626623</td>\n",
       "      <td>0.027438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.334575</td>\n",
       "      <td>0.033358</td>\n",
       "      <td>0.517287</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.017287</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.767518</td>\n",
       "      <td>0.016253</td>\n",
       "      <td>156.678314</td>\n",
       "      <td>1.391044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Finetuning</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.924101</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.954687</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.949752</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.971609</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018622</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.529690</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.029690</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.932361</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>43.585653</td>\n",
       "      <td>0.359795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cfk</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.923854</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.967722</td>\n",
       "      <td>0.008778</td>\n",
       "      <td>0.937581</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.959103</td>\n",
       "      <td>0.009163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031657</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.542481</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.042481</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.919447</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>35.515405</td>\n",
       "      <td>0.323679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FisherForgetting</td>\n",
       "      <td>1.00E-07</td>\n",
       "      <td>0.851990</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>0.881440</td>\n",
       "      <td>0.029847</td>\n",
       "      <td>0.856818</td>\n",
       "      <td>0.036456</td>\n",
       "      <td>0.902777</td>\n",
       "      <td>0.034580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054624</td>\n",
       "      <td>0.035992</td>\n",
       "      <td>0.536357</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.036357</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.889638</td>\n",
       "      <td>0.014001</td>\n",
       "      <td>1772.698107</td>\n",
       "      <td>43.972391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>eu_k</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.923983</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.970826</td>\n",
       "      <td>0.007023</td>\n",
       "      <td>0.938113</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.956468</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034761</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.524264</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.024264</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.937728</td>\n",
       "      <td>0.005156</td>\n",
       "      <td>169.670159</td>\n",
       "      <td>4.595021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Identity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.922775</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.970205</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>0.936366</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>0.954771</td>\n",
       "      <td>0.007138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.544419</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>0.044419</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>0.916969</td>\n",
       "      <td>0.007839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NegGrad</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.922812</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>0.970205</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>0.936346</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.954837</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.542016</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.042016</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.919390</td>\n",
       "      <td>0.006910</td>\n",
       "      <td>0.333553</td>\n",
       "      <td>0.009035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Scrub</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.919006</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0.944134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.929357</td>\n",
       "      <td>0.006294</td>\n",
       "      <td>0.971873</td>\n",
       "      <td>0.012296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008070</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.529380</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.029380</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.930123</td>\n",
       "      <td>0.003664</td>\n",
       "      <td>65.993787</td>\n",
       "      <td>1.643625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ssd</td>\n",
       "      <td>0.1_20</td>\n",
       "      <td>0.918942</td>\n",
       "      <td>0.004631</td>\n",
       "      <td>0.966480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.931356</td>\n",
       "      <td>0.004935</td>\n",
       "      <td>0.950988</td>\n",
       "      <td>0.006237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030416</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.542713</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.042713</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.916758</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>102.025570</td>\n",
       "      <td>0.797965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>unsir</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.924248</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.934823</td>\n",
       "      <td>0.007901</td>\n",
       "      <td>0.927159</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.991061</td>\n",
       "      <td>0.009304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013656</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.510775</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.010775</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.951349</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>43.023419</td>\n",
       "      <td>0.079245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>SuccessiveRandomLabels</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.921504</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.939789</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.924527</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.980793</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>0.503953</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.003953</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.956798</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>40.439231</td>\n",
       "      <td>0.319918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>salun</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.922583</td>\n",
       "      <td>0.001514</td>\n",
       "      <td>0.933582</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.925416</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.988946</td>\n",
       "      <td>0.006321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.512403</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.012403</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.948888</td>\n",
       "      <td>0.003498</td>\n",
       "      <td>42.109868</td>\n",
       "      <td>0.752978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 unlearner        lr  \\\n",
       "0                GoldModel       NaN   \n",
       "2          AdvancedNegGrad   0.00001   \n",
       "5              BadTeaching   0.00001   \n",
       "7               Finetuning    0.0001   \n",
       "10                     cfk    0.0001   \n",
       "12        FisherForgetting  1.00E-07   \n",
       "15                    eu_k      0.01   \n",
       "16                Identity       NaN   \n",
       "17                 NegGrad   0.00001   \n",
       "21                   Scrub     0.001   \n",
       "23                     ssd    0.1_20   \n",
       "27                   unsir     0.001   \n",
       "30  SuccessiveRandomLabels      0.01   \n",
       "34                   salun      0.01   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.test.unlearned  \\\n",
       "0                                        0.924166   \n",
       "2                                        0.731277   \n",
       "5                                        0.569609   \n",
       "7                                        0.924101   \n",
       "10                                       0.923854   \n",
       "12                                       0.851990   \n",
       "15                                       0.923983   \n",
       "16                                       0.922775   \n",
       "17                                       0.922812   \n",
       "21                                       0.919006   \n",
       "23                                       0.918942   \n",
       "27                                       0.924248   \n",
       "30                                       0.921504   \n",
       "34                                       0.922583   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.test.unlearned_std  \\\n",
       "0                                            0.000362   \n",
       "2                                            0.025033   \n",
       "5                                            0.030971   \n",
       "7                                            0.000492   \n",
       "10                                           0.001837   \n",
       "12                                           0.034140   \n",
       "15                                           0.001746   \n",
       "16                                           0.002523   \n",
       "17                                           0.002419   \n",
       "21                                           0.005136   \n",
       "23                                           0.004631   \n",
       "27                                           0.000453   \n",
       "30                                           0.000492   \n",
       "34                                           0.001514   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.forget.unlearned  \\\n",
       "0                                          0.936065   \n",
       "2                                          0.484171   \n",
       "5                                          0.601490   \n",
       "7                                          0.954687   \n",
       "10                                         0.967722   \n",
       "12                                         0.881440   \n",
       "15                                         0.970826   \n",
       "16                                         0.970205   \n",
       "17                                         0.970205   \n",
       "21                                         0.944134   \n",
       "23                                         0.966480   \n",
       "27                                         0.934823   \n",
       "30                                         0.939789   \n",
       "34                                         0.933582   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.forget.unlearned_std  \\\n",
       "0                                            0.006145     \n",
       "2                                            0.057938     \n",
       "5                                            0.039503     \n",
       "7                                            0.003511     \n",
       "10                                           0.008778     \n",
       "12                                           0.029847     \n",
       "15                                           0.007023     \n",
       "16                                           0.005267     \n",
       "17                                           0.005267     \n",
       "21                                           0.000000     \n",
       "23                                           0.000000     \n",
       "27                                           0.007901     \n",
       "30                                           0.000878     \n",
       "34                                           0.000878     \n",
       "\n",
       "    sklearn.metrics.accuracy_score.retain.unlearned  \\\n",
       "0                                          0.937864   \n",
       "2                                          0.748774   \n",
       "5                                          0.570408   \n",
       "7                                          0.949752   \n",
       "10                                         0.937581   \n",
       "12                                         0.856818   \n",
       "15                                         0.938113   \n",
       "16                                         0.936366   \n",
       "17                                         0.936346   \n",
       "21                                         0.929357   \n",
       "23                                         0.931356   \n",
       "27                                         0.927159   \n",
       "30                                         0.924527   \n",
       "34                                         0.925416   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.retain.unlearned_std       AUS   AUS_std  \\\n",
       "0                                            0.000149    0.989631  0.002816   \n",
       "2                                            0.027339    0.649319  0.038509   \n",
       "5                                            0.032986    0.626623  0.027438   \n",
       "7                                            0.000796    0.971609  0.000078   \n",
       "10                                           0.001223    0.959103  0.009163   \n",
       "12                                           0.036456    0.902777  0.034580   \n",
       "15                                           0.001064    0.956468  0.007314   \n",
       "16                                           0.001824    0.954771  0.007138   \n",
       "17                                           0.001843    0.954837  0.006944   \n",
       "21                                           0.006294    0.971873  0.012296   \n",
       "23                                           0.004935    0.950988  0.006237   \n",
       "27                                           0.000854    0.991061  0.009304   \n",
       "30                                           0.000323    0.980793  0.000676   \n",
       "34                                           0.002358    0.988946  0.006321   \n",
       "\n",
       "    ...  forget_diff  forget_diff_std      UMIA  UMIA_std  Unlearning_Score  \\\n",
       "0   ...     0.000000         0.000000  0.508837  0.000000          0.008837   \n",
       "2   ...     0.451893         0.051793  0.635659  0.019185          0.135659   \n",
       "5   ...     0.334575         0.033358  0.517287  0.000767          0.017287   \n",
       "7   ...     0.018622         0.002634  0.529690  0.002412          0.029690   \n",
       "10  ...     0.031657         0.002634  0.542481  0.000110          0.042481   \n",
       "12  ...     0.054624         0.035992  0.536357  0.003070          0.036357   \n",
       "15  ...     0.034761         0.000878  0.524264  0.006030          0.024264   \n",
       "16  ...     0.034140         0.000878  0.544419  0.006578          0.044419   \n",
       "17  ...     0.034140         0.000878  0.542016  0.005701          0.042016   \n",
       "21  ...     0.008070         0.006145  0.529380  0.001096          0.029380   \n",
       "23  ...     0.030416         0.006145  0.542713  0.002193          0.042713   \n",
       "27  ...     0.013656         0.003511  0.510775  0.001096          0.010775   \n",
       "30  ...     0.003724         0.005267  0.503953  0.000987          0.003953   \n",
       "34  ...     0.007449         0.000000  0.512403  0.002741          0.012403   \n",
       "\n",
       "    Unlearning_Score_std     nomus  nomus_std         time   time_std  \n",
       "0               0.000000  0.953246   0.000181  1598.794620  13.599261  \n",
       "2               0.019185  0.729980   0.031701   277.527701   1.103588  \n",
       "5               0.000767  0.767518   0.016253   156.678314   1.391044  \n",
       "7               0.002412  0.932361   0.002166    43.585653   0.359795  \n",
       "10              0.000110  0.919447   0.001028    35.515405   0.323679  \n",
       "12              0.003070  0.889638   0.014001  1772.698107  43.972391  \n",
       "15              0.006030  0.937728   0.005156   169.670159   4.595021  \n",
       "16              0.006578  0.916969   0.007839     0.000000   0.000000  \n",
       "17              0.005701  0.919390   0.006910     0.333553   0.009035  \n",
       "21              0.001096  0.930123   0.003664    65.993787   1.643625  \n",
       "23              0.002193  0.916758   0.000123   102.025570   0.797965  \n",
       "27              0.001096  0.951349   0.000870    43.023419   0.079245  \n",
       "30              0.000987  0.956798   0.000741    40.439231   0.319918  \n",
       "34              0.002741  0.948888   0.003498    42.109868   0.752978  \n",
       "\n",
       "[14 rows x 22 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each entry, do the mean and std and add to the relative unlearner in the best_df\n",
    "\n",
    "# add time and time_std to the best_df\n",
    "best_df['time'] = 0\n",
    "best_df['time_std'] = 0\n",
    "\n",
    "for i, row in best_df.iterrows():\n",
    "    name = row['unlearner']\n",
    "    if name != 'Identity': \n",
    "        best_df.loc[i, 'time'] = np.mean(times[name])\n",
    "        best_df.loc[i, 'time_std'] = np.std(times[name])\n",
    "    else:\n",
    "        best_df.loc[i, 'time'] = 0\n",
    "        best_df.loc[i, 'time_std'] = 0\n",
    "\n",
    "best_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unlearner</th>\n",
       "      <th>lr</th>\n",
       "      <th>sklearn.metrics.accuracy_score.test.unlearned</th>\n",
       "      <th>sklearn.metrics.accuracy_score.test.unlearned_std</th>\n",
       "      <th>sklearn.metrics.accuracy_score.forget.unlearned</th>\n",
       "      <th>sklearn.metrics.accuracy_score.forget.unlearned_std</th>\n",
       "      <th>sklearn.metrics.accuracy_score.retain.unlearned</th>\n",
       "      <th>sklearn.metrics.accuracy_score.retain.unlearned_std</th>\n",
       "      <th>AUS</th>\n",
       "      <th>AUS_std</th>\n",
       "      <th>...</th>\n",
       "      <th>forget_diff_std</th>\n",
       "      <th>UMIA</th>\n",
       "      <th>UMIA_std</th>\n",
       "      <th>Unlearning_Score</th>\n",
       "      <th>Unlearning_Score_std</th>\n",
       "      <th>nomus</th>\n",
       "      <th>nomus_std</th>\n",
       "      <th>time</th>\n",
       "      <th>time_std</th>\n",
       "      <th>speedup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GoldModel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.924166</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.936065</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.937864</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.989631</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.508837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.953246</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>1598.794620</td>\n",
       "      <td>13.599261</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdvancedNegGrad</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.731277</td>\n",
       "      <td>0.025033</td>\n",
       "      <td>0.484171</td>\n",
       "      <td>0.057938</td>\n",
       "      <td>0.748774</td>\n",
       "      <td>0.027339</td>\n",
       "      <td>0.649319</td>\n",
       "      <td>0.038509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051793</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>0.019185</td>\n",
       "      <td>0.135659</td>\n",
       "      <td>0.019185</td>\n",
       "      <td>0.729980</td>\n",
       "      <td>0.031701</td>\n",
       "      <td>277.527701</td>\n",
       "      <td>1.103588</td>\n",
       "      <td>5.760847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BadTeaching</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.569609</td>\n",
       "      <td>0.030971</td>\n",
       "      <td>0.601490</td>\n",
       "      <td>0.039503</td>\n",
       "      <td>0.570408</td>\n",
       "      <td>0.032986</td>\n",
       "      <td>0.626623</td>\n",
       "      <td>0.027438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033358</td>\n",
       "      <td>0.517287</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.017287</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.767518</td>\n",
       "      <td>0.016253</td>\n",
       "      <td>156.678314</td>\n",
       "      <td>1.391044</td>\n",
       "      <td>10.204313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Finetuning</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.924101</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.954687</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.949752</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.971609</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.529690</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.029690</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.932361</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>43.585653</td>\n",
       "      <td>0.359795</td>\n",
       "      <td>36.681672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cfk</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.923854</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.967722</td>\n",
       "      <td>0.008778</td>\n",
       "      <td>0.937581</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.959103</td>\n",
       "      <td>0.009163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.542481</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.042481</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.919447</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>35.515405</td>\n",
       "      <td>0.323679</td>\n",
       "      <td>45.016934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FisherForgetting</td>\n",
       "      <td>1.00E-07</td>\n",
       "      <td>0.851990</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>0.881440</td>\n",
       "      <td>0.029847</td>\n",
       "      <td>0.856818</td>\n",
       "      <td>0.036456</td>\n",
       "      <td>0.902777</td>\n",
       "      <td>0.034580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035992</td>\n",
       "      <td>0.536357</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.036357</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.889638</td>\n",
       "      <td>0.014001</td>\n",
       "      <td>1772.698107</td>\n",
       "      <td>43.972391</td>\n",
       "      <td>0.901899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>eu_k</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.923983</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.970826</td>\n",
       "      <td>0.007023</td>\n",
       "      <td>0.938113</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.956468</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.524264</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.024264</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.937728</td>\n",
       "      <td>0.005156</td>\n",
       "      <td>169.670159</td>\n",
       "      <td>4.595021</td>\n",
       "      <td>9.422957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Identity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.922775</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.970205</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>0.936366</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>0.954771</td>\n",
       "      <td>0.007138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.544419</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>0.044419</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>0.916969</td>\n",
       "      <td>0.007839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NegGrad</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.922812</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>0.970205</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>0.936346</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.954837</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.542016</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.042016</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.919390</td>\n",
       "      <td>0.006910</td>\n",
       "      <td>0.333553</td>\n",
       "      <td>0.009035</td>\n",
       "      <td>4793.227463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Scrub</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.919006</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0.944134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.929357</td>\n",
       "      <td>0.006294</td>\n",
       "      <td>0.971873</td>\n",
       "      <td>0.012296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.529380</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.029380</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.930123</td>\n",
       "      <td>0.003664</td>\n",
       "      <td>65.993787</td>\n",
       "      <td>1.643625</td>\n",
       "      <td>24.226441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ssd</td>\n",
       "      <td>0.1_20</td>\n",
       "      <td>0.918942</td>\n",
       "      <td>0.004631</td>\n",
       "      <td>0.966480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.931356</td>\n",
       "      <td>0.004935</td>\n",
       "      <td>0.950988</td>\n",
       "      <td>0.006237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.542713</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.042713</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.916758</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>102.025570</td>\n",
       "      <td>0.797965</td>\n",
       "      <td>15.670529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>unsir</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.924248</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.934823</td>\n",
       "      <td>0.007901</td>\n",
       "      <td>0.927159</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.991061</td>\n",
       "      <td>0.009304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.510775</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.010775</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.951349</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>43.023419</td>\n",
       "      <td>0.079245</td>\n",
       "      <td>37.161031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>SuccessiveRandomLabels</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.921504</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.939789</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.924527</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.980793</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>0.503953</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.003953</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.956798</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>40.439231</td>\n",
       "      <td>0.319918</td>\n",
       "      <td>39.535732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>salun</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.922583</td>\n",
       "      <td>0.001514</td>\n",
       "      <td>0.933582</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.925416</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.988946</td>\n",
       "      <td>0.006321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.512403</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.012403</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.948888</td>\n",
       "      <td>0.003498</td>\n",
       "      <td>42.109868</td>\n",
       "      <td>0.752978</td>\n",
       "      <td>37.967220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 unlearner        lr  \\\n",
       "0                GoldModel       NaN   \n",
       "2          AdvancedNegGrad   0.00001   \n",
       "5              BadTeaching   0.00001   \n",
       "7               Finetuning    0.0001   \n",
       "10                     cfk    0.0001   \n",
       "12        FisherForgetting  1.00E-07   \n",
       "15                    eu_k      0.01   \n",
       "16                Identity       NaN   \n",
       "17                 NegGrad   0.00001   \n",
       "21                   Scrub     0.001   \n",
       "23                     ssd    0.1_20   \n",
       "27                   unsir     0.001   \n",
       "30  SuccessiveRandomLabels      0.01   \n",
       "34                   salun      0.01   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.test.unlearned  \\\n",
       "0                                        0.924166   \n",
       "2                                        0.731277   \n",
       "5                                        0.569609   \n",
       "7                                        0.924101   \n",
       "10                                       0.923854   \n",
       "12                                       0.851990   \n",
       "15                                       0.923983   \n",
       "16                                       0.922775   \n",
       "17                                       0.922812   \n",
       "21                                       0.919006   \n",
       "23                                       0.918942   \n",
       "27                                       0.924248   \n",
       "30                                       0.921504   \n",
       "34                                       0.922583   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.test.unlearned_std  \\\n",
       "0                                            0.000362   \n",
       "2                                            0.025033   \n",
       "5                                            0.030971   \n",
       "7                                            0.000492   \n",
       "10                                           0.001837   \n",
       "12                                           0.034140   \n",
       "15                                           0.001746   \n",
       "16                                           0.002523   \n",
       "17                                           0.002419   \n",
       "21                                           0.005136   \n",
       "23                                           0.004631   \n",
       "27                                           0.000453   \n",
       "30                                           0.000492   \n",
       "34                                           0.001514   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.forget.unlearned  \\\n",
       "0                                          0.936065   \n",
       "2                                          0.484171   \n",
       "5                                          0.601490   \n",
       "7                                          0.954687   \n",
       "10                                         0.967722   \n",
       "12                                         0.881440   \n",
       "15                                         0.970826   \n",
       "16                                         0.970205   \n",
       "17                                         0.970205   \n",
       "21                                         0.944134   \n",
       "23                                         0.966480   \n",
       "27                                         0.934823   \n",
       "30                                         0.939789   \n",
       "34                                         0.933582   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.forget.unlearned_std  \\\n",
       "0                                            0.006145     \n",
       "2                                            0.057938     \n",
       "5                                            0.039503     \n",
       "7                                            0.003511     \n",
       "10                                           0.008778     \n",
       "12                                           0.029847     \n",
       "15                                           0.007023     \n",
       "16                                           0.005267     \n",
       "17                                           0.005267     \n",
       "21                                           0.000000     \n",
       "23                                           0.000000     \n",
       "27                                           0.007901     \n",
       "30                                           0.000878     \n",
       "34                                           0.000878     \n",
       "\n",
       "    sklearn.metrics.accuracy_score.retain.unlearned  \\\n",
       "0                                          0.937864   \n",
       "2                                          0.748774   \n",
       "5                                          0.570408   \n",
       "7                                          0.949752   \n",
       "10                                         0.937581   \n",
       "12                                         0.856818   \n",
       "15                                         0.938113   \n",
       "16                                         0.936366   \n",
       "17                                         0.936346   \n",
       "21                                         0.929357   \n",
       "23                                         0.931356   \n",
       "27                                         0.927159   \n",
       "30                                         0.924527   \n",
       "34                                         0.925416   \n",
       "\n",
       "    sklearn.metrics.accuracy_score.retain.unlearned_std       AUS   AUS_std  \\\n",
       "0                                            0.000149    0.989631  0.002816   \n",
       "2                                            0.027339    0.649319  0.038509   \n",
       "5                                            0.032986    0.626623  0.027438   \n",
       "7                                            0.000796    0.971609  0.000078   \n",
       "10                                           0.001223    0.959103  0.009163   \n",
       "12                                           0.036456    0.902777  0.034580   \n",
       "15                                           0.001064    0.956468  0.007314   \n",
       "16                                           0.001824    0.954771  0.007138   \n",
       "17                                           0.001843    0.954837  0.006944   \n",
       "21                                           0.006294    0.971873  0.012296   \n",
       "23                                           0.004935    0.950988  0.006237   \n",
       "27                                           0.000854    0.991061  0.009304   \n",
       "30                                           0.000323    0.980793  0.000676   \n",
       "34                                           0.002358    0.988946  0.006321   \n",
       "\n",
       "    ...  forget_diff_std      UMIA  UMIA_std  Unlearning_Score  \\\n",
       "0   ...         0.000000  0.508837  0.000000          0.008837   \n",
       "2   ...         0.051793  0.635659  0.019185          0.135659   \n",
       "5   ...         0.033358  0.517287  0.000767          0.017287   \n",
       "7   ...         0.002634  0.529690  0.002412          0.029690   \n",
       "10  ...         0.002634  0.542481  0.000110          0.042481   \n",
       "12  ...         0.035992  0.536357  0.003070          0.036357   \n",
       "15  ...         0.000878  0.524264  0.006030          0.024264   \n",
       "16  ...         0.000878  0.544419  0.006578          0.044419   \n",
       "17  ...         0.000878  0.542016  0.005701          0.042016   \n",
       "21  ...         0.006145  0.529380  0.001096          0.029380   \n",
       "23  ...         0.006145  0.542713  0.002193          0.042713   \n",
       "27  ...         0.003511  0.510775  0.001096          0.010775   \n",
       "30  ...         0.005267  0.503953  0.000987          0.003953   \n",
       "34  ...         0.000000  0.512403  0.002741          0.012403   \n",
       "\n",
       "    Unlearning_Score_std     nomus  nomus_std         time   time_std  \\\n",
       "0               0.000000  0.953246   0.000181  1598.794620  13.599261   \n",
       "2               0.019185  0.729980   0.031701   277.527701   1.103588   \n",
       "5               0.000767  0.767518   0.016253   156.678314   1.391044   \n",
       "7               0.002412  0.932361   0.002166    43.585653   0.359795   \n",
       "10              0.000110  0.919447   0.001028    35.515405   0.323679   \n",
       "12              0.003070  0.889638   0.014001  1772.698107  43.972391   \n",
       "15              0.006030  0.937728   0.005156   169.670159   4.595021   \n",
       "16              0.006578  0.916969   0.007839     0.000000   0.000000   \n",
       "17              0.005701  0.919390   0.006910     0.333553   0.009035   \n",
       "21              0.001096  0.930123   0.003664    65.993787   1.643625   \n",
       "23              0.002193  0.916758   0.000123   102.025570   0.797965   \n",
       "27              0.001096  0.951349   0.000870    43.023419   0.079245   \n",
       "30              0.000987  0.956798   0.000741    40.439231   0.319918   \n",
       "34              0.002741  0.948888   0.003498    42.109868   0.752978   \n",
       "\n",
       "        speedup  \n",
       "0      1.000000  \n",
       "2      5.760847  \n",
       "5     10.204313  \n",
       "7     36.681672  \n",
       "10    45.016934  \n",
       "12     0.901899  \n",
       "15     9.422957  \n",
       "16          inf  \n",
       "17  4793.227463  \n",
       "21    24.226441  \n",
       "23    15.670529  \n",
       "27    37.161031  \n",
       "30    39.535732  \n",
       "34    37.967220  \n",
       "\n",
       "[14 rows x 23 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate speedup with respect to gold model \n",
    "\n",
    "gold_time = best_df[best_df['unlearner'] == 'GoldModel']['time'].values[0]\n",
    "best_df['speedup'] = gold_time / best_df['time']\n",
    "best_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table*}[]\n",
      "\\centering\n",
      "\\resizebox{\\linewidth}{!}{\n",
      "\\begin{tabular}{l c c c c | c c | c c c | c | c}\n",
      "\\midrule\\hline \n",
      "\n",
      "DS & Unlearner & Test Acc \\up & Forget Acc \\up & Retain Acc \\up & AUS \\up & AIN \\down & Forget Diff \\down & UMIA \\down & Nomus \\up & Time (Speedup) \\down\\\\\n",
      "\\midrule\\hline \\\\ \n",
      "\n",
      "& Finetuning & \\underline{0.924} ± \\scriptsize{0.000} & 0.955 ± \\scriptsize{0.004} & \\textbf{0.950} ± \\scriptsize{0.001} & 0.972 ± \\scriptsize{0.000} & 1.000 ± \\scriptsize{0.000} & 0.019 ± \\scriptsize{0.003} & 0.530 ± \\scriptsize{0.002} & 0.932 ± \\scriptsize{0.002} & 43.586 ± \\scriptsize{0.4} (36.7) \\\\\n",
      "& SuccessiveRandomLabels & 0.922 ± \\scriptsize{0.000} & 0.940 ± \\scriptsize{0.001} & 0.925 ± \\scriptsize{0.000} & 0.981 ± \\scriptsize{0.001} & 1.387 ± \\scriptsize{0.078} & \\textbf{0.004} ± \\scriptsize{0.005} & \\textbf{0.504} ± \\scriptsize{0.001} & \\textbf{0.957} ± \\scriptsize{0.001} & 40.439 ± \\scriptsize{0.3} (39.5) \\\\\n",
      "& cfk & 0.924 ± \\scriptsize{0.002} & 0.968 ± \\scriptsize{0.009} & 0.938 ± \\scriptsize{0.001} & 0.959 ± \\scriptsize{0.009} & 4.647 ± \\scriptsize{6.568} & 0.032 ± \\scriptsize{0.003} & 0.542 ± \\scriptsize{0.000} & 0.919 ± \\scriptsize{0.001} & \\underline{35.515} ± \\scriptsize{0.3} (45.0) \\\\\n",
      "& eu_k & 0.924 ± \\scriptsize{0.002} & \\textbf{0.971} ± \\scriptsize{0.007} & \\underline{0.938} ± \\scriptsize{0.001} & 0.956 ± \\scriptsize{0.007} & \\underline{0.501} ± \\scriptsize{0.704} & 0.035 ± \\scriptsize{0.001} & 0.524 ± \\scriptsize{0.006} & 0.938 ± \\scriptsize{0.005} & 169.670 ± \\scriptsize{4.6} (9.4) \\\\\n",
      "& NegGrad & 0.923 ± \\scriptsize{0.002} & \\underline{0.970} ± \\scriptsize{0.005} & 0.936 ± \\scriptsize{0.002} & 0.955 ± \\scriptsize{0.007} & \\textbf{0.004} ± \\scriptsize{0.001} & 0.034 ± \\scriptsize{0.001} & 0.542 ± \\scriptsize{0.006} & 0.919 ± \\scriptsize{0.007} & \\textbf{0.334} ± \\scriptsize{0.0} (4793.2) \\\\\n",
      "& AdvancedNegGrad & 0.731 ± \\scriptsize{0.025} & 0.484 ± \\scriptsize{0.058} & 0.749 ± \\scriptsize{0.027} & 0.649 ± \\scriptsize{0.039} & 38.736 ± \\scriptsize{7.792} & 0.452 ± \\scriptsize{0.052} & 0.636 ± \\scriptsize{0.019} & 0.730 ± \\scriptsize{0.032} & 277.528 ± \\scriptsize{1.1} (5.8) \\\\\n",
      "& unsir & \\textbf{0.924} ± \\scriptsize{0.000} & 0.935 ± \\scriptsize{0.008} & 0.927 ± \\scriptsize{0.001} & \\textbf{0.991} ± \\scriptsize{0.009} & 1.166 ± \\scriptsize{0.235} & 0.014 ± \\scriptsize{0.004} & \\underline{0.511} ± \\scriptsize{0.001} & \\underline{0.951} ± \\scriptsize{0.001} & 43.023 ± \\scriptsize{0.1} (37.2) \\\\\n",
      "& BadTeaching & 0.570 ± \\scriptsize{0.031} & 0.601 ± \\scriptsize{0.040} & 0.570 ± \\scriptsize{0.033} & 0.627 ± \\scriptsize{0.027} & 6.641 ± \\scriptsize{5.158} & 0.335 ± \\scriptsize{0.033} & 0.517 ± \\scriptsize{0.001} & 0.768 ± \\scriptsize{0.016} & 156.678 ± \\scriptsize{1.4} (10.2) \\\\\n",
      "& Scrub & 0.919 ± \\scriptsize{0.005} & 0.944 ± \\scriptsize{0.000} & 0.929 ± \\scriptsize{0.006} & 0.972 ± \\scriptsize{0.012} & 0.779 ± \\scriptsize{0.157} & 0.008 ± \\scriptsize{0.006} & 0.529 ± \\scriptsize{0.001} & 0.930 ± \\scriptsize{0.004} & 65.994 ± \\scriptsize{1.6} (24.2) \\\\\n",
      "& FisherForgetting & 0.852 ± \\scriptsize{0.034} & 0.881 ± \\scriptsize{0.030} & 0.857 ± \\scriptsize{0.036} & 0.903 ± \\scriptsize{0.035} & 0.779 ± \\scriptsize{0.157} & 0.055 ± \\scriptsize{0.036} & 0.536 ± \\scriptsize{0.003} & 0.890 ± \\scriptsize{0.014} & 1772.698 ± \\scriptsize{44.0} (0.9) \\\\\n",
      "& ssd & 0.919 ± \\scriptsize{0.005} & 0.966 ± \\scriptsize{0.000} & 0.931 ± \\scriptsize{0.005} & 0.951 ± \\scriptsize{0.006} & 0.890 ± \\scriptsize{0.626} & 0.030 ± \\scriptsize{0.006} & 0.543 ± \\scriptsize{0.002} & 0.917 ± \\scriptsize{0.000} & 102.026 ± \\scriptsize{0.8} (15.7) \\\\\n",
      "& salun & 0.923 ± \\scriptsize{0.002} & 0.934 ± \\scriptsize{0.001} & 0.925 ± \\scriptsize{0.002} & \\underline{0.989} ± \\scriptsize{0.006} & 2.881 ± \\scriptsize{0.781} & \\underline{0.007} ± \\scriptsize{0.000} & 0.512 ± \\scriptsize{0.003} & 0.949 ± \\scriptsize{0.003} & 42.110 ± \\scriptsize{0.8} (38.0) \\\\\n",
      "& Identity & 0.923 ± \\scriptsize{0.003} & \\underline{0.970} ± \\scriptsize{0.005} & 0.936 ± \\scriptsize{0.002} & 0.955 ± \\scriptsize{0.007} & \\textbf{0.004} ± \\scriptsize{0.001} & 0.034 ± \\scriptsize{0.001} & 0.544 ± \\scriptsize{0.007} & 0.917 ± \\scriptsize{0.008} & 0.000 ± \\scriptsize{0.0} (inf) \\\\\n",
      "& GoldModel & 0.924 ± \\scriptsize{0.000} & 0.936 ± \\scriptsize{0.006} & 0.938 ± \\scriptsize{0.000} & 0.990 ± \\scriptsize{0.003} & 0.779 ± \\scriptsize{0.157} & 0.000 ± \\scriptsize{0.000} & 0.509 ± \\scriptsize{0.000} & 0.953 ± \\scriptsize{0.000} & 1598.795 ± \\scriptsize{13.6} (1.0) \\\\\n",
      "\\midrule\\hline \n",
      "\n",
      "\\end{tabular}}\n",
      "\\caption{Experimental results of different unlearning methods.}\n",
      "\\label{tab:unlearning_results}\n",
      "\\end{table*}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def highlight_best_and_second_best(df, column, ascending=True):\n",
    "    filtered_df = df[~df['unlearner'].isin(['GoldModel', 'Identity'])]\n",
    "    sorted_df = filtered_df.sort_values(by=column, ascending=ascending)\n",
    "    best_value = sorted_df.iloc[0][column]\n",
    "    second_best_value = sorted_df.iloc[1][column]\n",
    "    return best_value, second_best_value\n",
    "\n",
    "def format_value(value, best_value, second_best_value):\n",
    "    if value == best_value:\n",
    "        return f\"\\\\textbf{{{value:.3f}}}\"\n",
    "    elif value == second_best_value:\n",
    "        return f\"\\\\underline{{{value:.3f}}}\"\n",
    "    else:\n",
    "        return f\"{value:.3f}\"\n",
    "\n",
    "def df_to_latex_table(df):\n",
    "    # Define the LaTeX table header\n",
    "    header = r\"\"\"\\begin{table*}[]\n",
    "\\centering\n",
    "\\resizebox{\\linewidth}{!}{\n",
    "\\begin{tabular}{l c c c c | c c | c c c | c | c}\n",
    "\\midrule\\hline \n",
    "\n",
    "DS & Unlearner & Test Acc \\up & Forget Acc \\up & Retain Acc \\up & AUS \\up & AIN \\down & Forget Diff \\down & UMIA \\down & Nomus \\up & Time (Speedup) \\down\\\\\n",
    "\\midrule\\hline \\\\ \n",
    "\"\"\"\n",
    "\n",
    "    # Get the best and second best values for each column\n",
    "    best_test_acc, second_best_test_acc = highlight_best_and_second_best(df, 'sklearn.metrics.accuracy_score.test.unlearned', ascending=False)\n",
    "    best_forget_acc, second_best_forget_acc = highlight_best_and_second_best(df, 'sklearn.metrics.accuracy_score.forget.unlearned', ascending=False)\n",
    "    best_retain_acc, second_best_retain_acc = highlight_best_and_second_best(df, 'sklearn.metrics.accuracy_score.retain.unlearned', ascending=False)\n",
    "    best_aus, second_best_aus = highlight_best_and_second_best(df, 'AUS', ascending=False)\n",
    "    best_ain, second_best_ain = highlight_best_and_second_best(df, 'AIN', ascending=True)\n",
    "    best_forget_diff, second_best_forget_diff = highlight_best_and_second_best(df, 'forget_diff', ascending=True)\n",
    "    best_umia, second_best_umia = highlight_best_and_second_best(df, 'UMIA', ascending=True)\n",
    "    best_nomus, second_best_nomus = highlight_best_and_second_best(df, 'nomus', ascending=False)\n",
    "    best_time, second_best_time = highlight_best_and_second_best(df, 'time', ascending=True)\n",
    "\n",
    "    # Format the data into LaTeX table rows\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Convert values into \"mean ± std\" format\n",
    "        test_acc = f\"{format_value(row['sklearn.metrics.accuracy_score.test.unlearned'], best_test_acc, second_best_test_acc)} ± \\scriptsize{{{row['sklearn.metrics.accuracy_score.test.unlearned_std']:.3f}}}\"\n",
    "        forget_acc = f\"{format_value(row['sklearn.metrics.accuracy_score.forget.unlearned'], best_forget_acc, second_best_forget_acc)} ± \\scriptsize{{{row['sklearn.metrics.accuracy_score.forget.unlearned_std']:.3f}}}\"\n",
    "        retain_acc = f\"{format_value(row['sklearn.metrics.accuracy_score.retain.unlearned'], best_retain_acc, second_best_retain_acc)} ± \\scriptsize{{{row['sklearn.metrics.accuracy_score.retain.unlearned_std']:.3f}}}\"\n",
    "        aus = f\"{format_value(row['AUS'], best_aus, second_best_aus)} ± \\scriptsize{{{row['AUS_std']:.3f}}}\"\n",
    "        ain = f\"{format_value(row['AIN'], best_ain, second_best_ain)} ± \\scriptsize{{{row['AIN_std']:.3f}}}\"\n",
    "        forget_diff = f\"{format_value(row['forget_diff'], best_forget_diff, second_best_forget_diff)} ± \\scriptsize{{{row['forget_diff_std']:.3f}}}\"\n",
    "        umia = f\"{format_value(row['UMIA'], best_umia, second_best_umia)} ± \\scriptsize{{{row['UMIA_std']:.3f}}}\"\n",
    "        nomus = f\"{format_value(row['nomus'], best_nomus, second_best_nomus)} ± \\scriptsize{{{row['nomus_std']:.3f}}}\"\n",
    "        time = f\"{format_value(row['time'], best_time, second_best_time)} ± \\scriptsize{{{row['time_std']:.1f}}}\"\n",
    "        speedup = f\"{row['speedup']:.1f}\"\n",
    "        lr = row['lr']\n",
    "\n",
    "        # Format the row\n",
    "        rows.append(f\"& {row['unlearner']} & {test_acc} & {forget_acc} & {retain_acc} & {aus} & {ain} & {forget_diff} & {umia} & {nomus} & {time} ({speedup}) \\\\\\\\\")\n",
    "\n",
    "    # Define the LaTeX table footer\n",
    "    footer = r\"\"\"\\midrule\\hline \n",
    "\n",
    "\\end{tabular}}\n",
    "\\caption{Experimental results of different unlearning methods.}\n",
    "\\label{tab:unlearning_results}\n",
    "\\end{table*}\"\"\"\n",
    "\n",
    "    # Combine header, rows, and footer\n",
    "    latex_table = \"\\n\".join([header] + rows + [footer])\n",
    "\n",
    "    return latex_table\n",
    "\n",
    "best_df['unlearner'] = pd.Categorical(best_df['unlearner'], ['Finetuning', 'SuccessiveRandomLabels', 'cfk', 'eu_k', 'NegGrad', 'AdvancedNegGrad', 'unsir', 'BadTeaching', 'Scrub', 'FisherForgetting', 'ssd', 'salun', 'Identity', 'GoldModel'])\n",
    "\n",
    "best_df = best_df.sort_values('unlearner')\n",
    "\n",
    "latex_code = df_to_latex_table(best_df)\n",
    "\n",
    "print(latex_code)  # Print LaTeX output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertModel\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hf_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hf_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hf_env/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1077\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m-> 1077\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1086\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((batch_size, seq_length \u001b[38;5;241m+\u001b[39m past_key_values_length), device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/hf_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hf_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hf_env/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:210\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    207\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[1;32m    213\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
      "File \u001b[0;32m~/miniconda3/envs/hf_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hf_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hf_env/lib/python3.12/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hf_env/lib/python3.12/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# import a bert tokenizer \n",
    "from transformers import BertTokenizer\n",
    "import torch \n",
    "\n",
    "input = torch.tensor([[2.4, 1.2], [1.2, 3.4]])\n",
    "print(input.shape)\n",
    "input_attention_mask = torch.tensor([1, 1])\n",
    "\n",
    "# create a model \n",
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "output = model(input, attention_mask=input_attention_mask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
